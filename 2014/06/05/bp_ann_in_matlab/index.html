<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>An implementation of Neural Network with Back Propagation in MATLAB | Flight log</title>
  <meta name="author" content="Hugo Feng">
  
  <meta name="description" content="About code, machine learning, and life">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">

  <meta property="og:title" content="An implementation of Neural Network with Back Propagation in MATLAB"/>
  <meta property="og:site_name" content="Flight log"/>

  
    <meta property="og:image" content="undefined"/>
  

  <link href="/favicon.png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="Flight log" type="application/atom+xml">

  <link rel="stylesheet" href="//netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css"> 
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">

  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  <script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
  
<script type="text/javascript">
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-50169885-1', 'hugofeng.info');
  ga('send', 'pageview');
</script>


</head>


<body>
<div class="container">
    <div class="row">
        <div class="header">
    <ul class="nav nav-pills pull-right">
        <li>
            <div>
                <ol class="breadcrumb">
                    
						<li><a href="/">Home</a></li>
                    
						<li><a href="/archives">Archives</a></li>
                    
						<li><a href="/about">About</a></li>
                    

					<!--%- partial('post/blogroll') %--> 

                    <li> <a href="/atom.xml">Rss</a> </li>
                </ol>
            </div>
        </li>
    </ul>

    <h1>
        <a href="/">Flight log</a> 
    </h1>
    <h5 class="text-muted"> About code, machine learning and part of Hugo&#39;s life.  </h5>
</div>

    </div>

    <hr> 

    <div class="row">
        <!--Body content-->
		<article class="post">
  
    <div class="gallery">
  <div class="photoset">
    
      <img src="">
    
  </div>
  <div class="control">
    <div class="prev"></div>
    <div class="next"></div>
  </div>
</div>
  
  <div class="post-content">
    <div class="header">
      
  
    <h2 class="title">An implementation of Neural Network with Back Propagation in MATLAB
        <small>
            <time datetime="2014-06-05T20:45:14.000Z">Jun 5 2014</time>
        </small>
    </h2>
  


    </div>

    <div class="entry">
      
        <h3 id="Test_the_program_with_a_data_set">Test the program with a data set</h3>
<ul>
<li><strong>Network attributes</strong>: 1 hidden layer with 3 neurons, initial biases and weights are set to random from -1 to 1, with 0.5 learning rate</li>
<li><strong>Target</strong>: XOR(x1, x2)</li>
<li><strong>Training</strong> set: [x1, x2] = [0 0; 0 1; 1 0; 1 1], y = [0; 1; 1; 0]</li>
<li><strong>Test set</strong>: (Same as training set),   error = (sum(errors .^ 2))^0.5</li>
</ul>
<h4 id="Output">Output</h4>
<p><img src="/img/bp1.png" alt="Standard diviation of errors with iterations"><br><img src="/img/bp2.png" alt="Program output"><br><img src="/img/bp3.png" alt="Output of trained Neural Network"></p>
<a id="more"></a>

<h3 id="Analyze">Analyze</h3>
<p>Since neural network with activation function Sigmoid, is a highly non-linear system, the formation of the network can be depended on the initial value of weights and biases, and its non-linear feature may grow with the number of layers as well as number of neurons. So sometimes, the following network formation can also be trained from the same training set and settings.</p>
<p><img src="/img/bp4.png" alt="Another trained Network1"><br><img src="/img/bp5.png" alt="Another trained Network2"></p>
<h3 id="Source_Code_in_MATLAB">Source Code in MATLAB</h3>
<figure class="highlight MATLAB"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div></pre></td><td class="code"><pre><div class="line">%% BPANN: Artificial Neural Network with Back Propagation</div><div class="line">%% Author: Xuyang Feng</div><div class="line">function  BPANN()</div><div class="line"></div><div class="line">    %---Set training parameters</div><div class="line">    iterations = 5000;</div><div class="line">    errorThreshhold = 0.1;</div><div class="line">    learningRate = 0.5;</div><div class="line">    %---Set hidden layer type, for example: [4, 3, 2]</div><div class="line">    hiddenNeurons = [3 2];</div><div class="line"></div><div class="line">    </div><div class="line">    %---'Xor' training data</div><div class="line">    trainInp = [0 0; 0 1; 1 0; 1 1];</div><div class="line">    trainOut = [0; 1; 1; 0];</div><div class="line">    testInp = trainInp;</div><div class="line">    testRealOut = trainOut;</div><div class="line">    </div><div class="line"></div><div class="line">    % %---'And' training data</div><div class="line">    % trainInp = [1 1; 1 0; 0 1; 0 0];</div><div class="line">    % trainOut = [1; 0; 0; 0];</div><div class="line">    % testInp = trainInp;</div><div class="line">    % testRealOut = trainOut;</div><div class="line"></div><div class="line">    assert(size(trainInp,1)==size(trainOut, 1),...</div><div class="line">        'Counted different sets of input and output.');</div><div class="line"></div><div class="line">    %---Initialize Network attributes</div><div class="line">    inArgc = size(trainInp, 2);</div><div class="line">    outArgc = size(trainOut, 2);</div><div class="line">    trainsetCount = size(trainInp, 1);</div><div class="line">    </div><div class="line">    %---Add output layer</div><div class="line">    layerOfNeurons = [hiddenNeurons, outArgc];</div><div class="line">    layerCount = size(layerOfNeurons, 2);</div><div class="line">    </div><div class="line">    %---Weight and bias random range</div><div class="line">    e = 1;</div><div class="line">    b = -e;</div><div class="line"></div><div class="line">    %---Set initial random weights</div><div class="line">    weightCell = cell(1, layerCount);</div><div class="line">    for i = 1:layerCount</div><div class="line">        if i == 1</div><div class="line">            weightCell{1} = unifrnd(b, e, inArgc,layerOfNeurons(1));</div><div class="line">        else</div><div class="line">            weightCell{i} = unifrnd(b, e, layerOfNeurons(i-1),layerOfNeurons(i));</div><div class="line">        end</div><div class="line">    end</div><div class="line"></div><div class="line">    %---Set initial biases</div><div class="line">    biasCell = cell(1, layerCount);</div><div class="line">    for i = 1:layerCount</div><div class="line">        biasCell{i} = unifrnd(b, e, 1, layerOfNeurons(i));</div><div class="line">    end</div><div class="line"></div><div class="line"></div><div class="line">    %----------------------</div><div class="line">    %---Begin training</div><div class="line">    %----------------------</div><div class="line">    for iter = 1:iterations</div><div class="line">        for i = 1:trainsetCount</div><div class="line">            % choice = randi([1 trainsetCount]);</div><div class="line">            choice = i;</div><div class="line">            sampleIn = trainInp(choice, :);</div><div class="line">            sampleTarget = trainOut(choice, :);</div><div class="line"></div><div class="line">            [realOutput, layerOutputCells] = ForwardNetwork(sampleIn, layerOfNeurons, weightCell, biasCell);</div><div class="line">            [weightCell, biasCell] = BackPropagate(learningRate, sampleIn, realOutput, sampleTarget, layerOfNeurons, ...</div><div class="line">                weightCell, biasCell, layerOutputCells);</div><div class="line">        end</div><div class="line"></div><div class="line">        %plot overall network error at end of each iteration</div><div class="line">        error = zeros(trainsetCount, outArgc);</div><div class="line">        for t = 1:trainsetCount</div><div class="line">            [predict, layeroutput] = ForwardNetwork(trainInp(t, :), layerOfNeurons, weightCell, biasCell);</div><div class="line">            p(t) = predict;</div><div class="line">            error(t, : ) = predict - trainOut(t, :);</div><div class="line">        end</div><div class="line"></div><div class="line">        err(iter) = (sum(error.^2)/trainsetCount)^0.5;</div><div class="line">        figure(1);</div><div class="line">        plot(err);</div><div class="line"></div><div class="line">        %---Stop if reach error threshold</div><div class="line">        if err(iter) &lt; errorThreshhold</div><div class="line">            break;</div><div class="line">        end</div><div class="line">    end</div><div class="line">    </div><div class="line">    %--Test the trained network with a test set</div><div class="line">    testsetCount = size(testInp, 1);</div><div class="line">    error = zeros(testsetCount, outArgc);</div><div class="line">    for t = 1:testsetCount</div><div class="line">        [predict, layeroutput] = ForwardNetwork(testInp(t, :), layerOfNeurons, weightCell, biasCell);</div><div class="line">        p(t) = predict;</div><div class="line">        error(t, : ) = predict - testRealOut(t, :);</div><div class="line">    end</div><div class="line"></div><div class="line">    %---Print predictions</div><div class="line">    fprintf('Ended with %d iterations.\n', iter);</div><div class="line">    a = testInp;</div><div class="line">    b = testRealOut;</div><div class="line">    c = p';</div><div class="line">    x1_x2_act_pred_err = [a b c c-b]</div><div class="line"></div><div class="line">    %---Plot Surface of network predictions</div><div class="line">    testInpx1 = [-1:0.1:1];</div><div class="line">    testInpx2 = [-1:0.1:1];</div><div class="line">    [X1, X2] = meshgrid(testInpx1, testInpx2);</div><div class="line">    testOutRows = size(X1, 1);</div><div class="line">    testOutCols = size(X1, 2);</div><div class="line">    testOut = zeros(testOutRows, testOutCols);</div><div class="line">    for row = [1:testOutRows]</div><div class="line">        for col = [1:testOutCols]</div><div class="line">            test = [X1(row, col), X2(row, col)];</div><div class="line">            [out, l] = ForwardNetwork(test, layerOfNeurons, weightCell, biasCell);</div><div class="line">            testOut(row, col) = out;</div><div class="line">        end</div><div class="line">    end</div><div class="line">    figure(2);</div><div class="line">    surf(X1, X2, testOut);</div><div class="line"></div><div class="line">end</div><div class="line"></div><div class="line">%% BackPropagate: Backpropagate the output through the network and adjust weights and biases</div><div class="line">function [weightCell, biasCell] = BackPropagate(rate, in, realOutput, sampleTarget, layer, weightCell, biasCell, layerOutputCells)</div><div class="line">    layerCount = size(layer, 2);</div><div class="line">    delta = cell(1, layerCount);</div><div class="line">    D_weight = cell(1, layerCount);</div><div class="line">    D_bias = cell(1, layerCount);</div><div class="line">    %---From Output layer, it has different formula</div><div class="line">    output = layerOutputCells{layerCount};</div><div class="line">    delta{layerCount} = output .* (1-output) .* (sampleTarget - output);</div><div class="line">    preoutput = layerOutputCells{layerCount-1};</div><div class="line">    D_weight{layerCount} = rate .* preoutput' * delta{layerCount};</div><div class="line">    D_bias{layerCount} = rate .* delta{layerCount};</div><div class="line"></div><div class="line">    %---Back propagate for Hidden layers</div><div class="line">    for layerIndex = layerCount-1:-1:1</div><div class="line">        output = layerOutputCells{layerIndex};</div><div class="line">        if layerIndex == 1</div><div class="line">            preoutput = in;</div><div class="line">        else</div><div class="line">            preoutput = layerOutputCells{layerIndex-1};</div><div class="line">        end</div><div class="line"></div><div class="line">        weight = weightCell{layerIndex+1};</div><div class="line">        sumup = (weight * delta{layerIndex+1}')';</div><div class="line">        delta{layerIndex} = output .* (1 - output) .* sumup;</div><div class="line"></div><div class="line">        D_weight{layerIndex} = rate .* preoutput' * delta{layerIndex};</div><div class="line">        D_bias{layerIndex} = rate .* delta{layerIndex};</div><div class="line">    end</div><div class="line">    %---Update weightCell and biasCell</div><div class="line">    for layerIndex = 1:layerCount</div><div class="line">        weightCell{layerIndex} = weightCell{layerIndex} + D_weight{layerIndex};</div><div class="line">        biasCell{layerIndex} = biasCell{layerIndex} + D_bias{layerIndex};</div><div class="line">    end</div><div class="line"></div><div class="line"></div><div class="line">end</div><div class="line"></div><div class="line"></div><div class="line">%% ForwardNetwork: Compute feed forward neural network, Return the output and output of each neuron in each layer</div><div class="line">function [realOutput, layerOutputCells] = ForwardNetwork(in, layer, weightCell, biasCell)</div><div class="line">    layerCount = size(layer, 2);</div><div class="line">    layerOutputCells = cell(1, layerCount);</div><div class="line"></div><div class="line">    out = in;</div><div class="line">    for layerIndex = 1:layerCount</div><div class="line">        X = out;</div><div class="line">        bias = biasCell{layerIndex};</div><div class="line">        out = Sigmoid(X * weightCell{layerIndex} + bias);</div><div class="line">        layerOutputCells{layerIndex} = out;</div><div class="line">    end</div><div class="line">    realOutput = out;    </div><div class="line">end</div></pre></td></tr></table></figure>

      
    </div>

	

	<hr/>

	
		<nav id="pagination">
			
				<a href="/2014/08/18/Auto_Timestamp/" class="alignleft prev" title="Fast Copying Timestamp on Mac">Fast Copying Timestamp on Mac</a>
			
			
				<a href="/2014/04/25/float-operations-on-arm/" class="alignright next" title="Floating point operations on ARM processor">Floating point operations on ARM processor</a>
			
		</nav>
	
  </div>
</article>


<section id="comment">
  <h1 class="title">Comments</h1>
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>


    </div>

    <div class="footer">
        <div class="alignleft">
  
	  2015 &copy; Hugo Feng,
  
  All rights reserved. Powered by <a href="http://zespia.tw/hexo/" title="Hexo">Hexo</a>. Hosted on <a href="https://github.com/HugoFeng/hugofeng.github.io" title="GitHub">GitHub</a>.
</div>
<div class="clearfix"></div>

    </div>

</div>

<script src="//netdna.bootstrapcdn.com/bootstrap/3.1.1/js/bootstrap.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>


<script type="text/javascript">
var disqus_shortname = 'hugofeng';

(function(){
  var dsq = document.createElement('script');
  dsq.type = 'text/javascript';
  dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
}());
</script>



<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
    (function($){
        $('.fancybox').fancybox();
    })(jQuery);
</script>




<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</body>
</html>
