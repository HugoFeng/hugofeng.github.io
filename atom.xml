<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[Flight log]]></title>
  <subtitle><![CDATA[About code, machine learning and part of Hugo's life.]]></subtitle>
  <link href="/atom.xml" rel="self"/>
  <link href="http://hugofeng.info/"/>
  <updated>2014-08-28T18:49:41.478Z</updated>
  <id>http://hugofeng.info/</id>
  
  <author>
    <name><![CDATA[Hugo Feng]]></name>
    <email><![CDATA[hugo.fxy@gmail.com]]></email>
  </author>
  
  <generator uri="http://zespia.tw/hexo/">Hexo</generator>
  
  <entry>
    <title><![CDATA[Lambda and Stream in Java 8]]></title>
    <link href="http://hugofeng.info/2014/08/28/java8_lambda_stream/"/>
    <id>http://hugofeng.info/2014/08/28/java8_lambda_stream/</id>
    <published>2014-08-28T18:05:57.000Z</published>
    <updated>2014-08-28T18:48:19.000Z</updated>
    <content type="html"><![CDATA[<h3 id="1-_Introduction">1. Introduction</h3>
<p> This article introduces the new functional-programming features supported in Java 8. It first talks about the basic ways to implement lambdas and streams in the latest version of JDK, and then, pros and cons of this new feature are analyzed. </p>
<p> We assume you are already familiar with functional programming styles and have dealt with lambdas and streams in other languages. So in this article we are not going to introduce these things in conceptual level, and some of them are compared with Scheme.</p>
<h3 id="2-_What_is_a_lambda_and_a_stream_in_Java_8?">2. What is a lambda and a stream in Java 8?</h3>
<h4 id="2-1_Lambda_expression">2.1 Lambda expression</h4>
<p>Java 8 provides a new package <code>java.util.function</code> containing functional interfaces, and a lambda expression is an instance of a subtype of <code>Object</code> implementing one of the functional interfaces. Consider this as &#x2018;Syntactic Sugar&#x2019; mapping lambda expressions syntaxes to an anonymous class. </p>
<p>For example:</p>
<ul>
<li>Old Java style using anonymous inner class</li>
</ul>
<figure class="highlight"><table><tr><td class="gutter"><pre>1
2
3
4
5
</pre></td><td class="code"><pre>Thread t1 = <span class="keyword">new</span> Thread(<span class="keyword">new</span> Runnable() {     
            <span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span>() {
                System.<span class="keyword">out</span>.println(<span class="string">&quot;Hey there anonymous!&quot;</span>);
            }
        });
</pre></td></tr></table></figure>

<ul>
<li>Java 8 lambda style</li>
</ul>
<figure class="highlight"><table><tr><td class="gutter"><pre>1
2
3
</pre></td><td class="code"><pre>Thread t2 = <span class="keyword">new</span> Thread(
                    <span class="function"><span class="params">()</span>-&gt;</span>{System.out.println(<span class="string">&quot;Hey lambda!&quot;</span>);} 
                );
</pre></td></tr></table></figure>

<p>The syntax of a lambda function is like &#x201C;()-&gt;{&#x2026;}&#x201D;, where the argument are put in brackets, followed by an arrow alike sign &#x201C;-&gt;&#x201D;, with the body of the lambda as the last part. </p>
<p>There are following categories [1] of interfaces declared in <code>java.util.function</code> package:</p>
<ul>
<li><strong>Function</strong>: Takes a single parameter, returns result based on parameter value</li>
<li><strong>Predicate</strong>: Takes a single parameter, returns a boolean result based on parameter value</li>
<li><strong>BiFunction</strong>: Takes two parameters, returns result based on parameter value</li>
<li><strong>Supplier</strong>: Takes no parameters, returns a result</li>
<li><strong>Consumer</strong>: Takes a single parameter with no return (void)</li>
</ul>
<p>Beside these, one can also declare their own functional interfaces. Generally, the new java compiler accepts all interfaces declared with exactly one method as a functional interface, but one can also explicitly declare one by adding <code>@FunctionalInterface</code> annotation. For example:</p>
<figure class="highlight"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
</pre></td><td class="code"><pre><span class="keyword">public</span> <span class="keyword">class</span> Test1 {
    @FunctionalInterface  <span class="comment">// This is optional.</span>
    <span class="keyword">public</span> <span class="keyword">interface</span> Discount { <span class="keyword">double</span> apply(<span class="keyword">double</span> originalPrice); }
    
    Discount discount;
    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setDiscount</span>(Discount d){ discount = d; }
    
    <span class="keyword">public</span> <span class="keyword">double</span> <span class="title">getPriceAfterDiscount</span>(<span class="keyword">double</span> price){ <span class="keyword">return</span> discount.apply(price); }

    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span>(String[] args) {
        Test1 test1 = <span class="keyword">new</span> Test1();
        <span class="keyword">double</span> applePrice = <span class="number">1.0</span>;

        <span class="comment">// With type inference, compiler will expand this lambda to </span>
        <span class="comment">// an instance of anonymous class implementing Discount interface</span>
        test1.setDiscount(p -&gt; <span class="number">0.8</span>*p); 

        <span class="keyword">double</span> applePriceWithDiscount = test1. getPriceAfterDiscount(applePrice);
        System.<span class="keyword">out</span>.println(applePriceWithDiscount); <span class="comment">// 0.8</span>
    }
    
}
</pre></td></tr></table></figure>

<h4 id="2-2_Stream">2.2 Stream</h4>
<p>Stream API is provided in <code>java.util.stream</code> package. Stream is built on the basis of lambda with lazy evaluation feature. It is an enhancement made to collection interfaces. It is extremely handy for processing data.</p>
<p>Due to backward compatibility reasons, simply adding additional methods to an interface will require rewriting all the classes implementing this interface. So in order to add additional features to old <code>Collection</code> Class, Java 8 introduces <code>default method</code>. <code>Default method</code> is an default implementation of a method, and is defined in the original class of the interface. With the help of <code>default methods</code>, subtypes of <code>Collection</code> class, classes user defined with old version Java, can work without realizing the added interfaces explicitly, so that adding new features to old classes will be invisible to users.</p>
<p><code>Stream</code> is a sequence of elements supporting sequential and parallel aggregate operations. It&#x2019;s not a data structure so it doesn&#x2019;t store anything. It takes a lambda expression as an argument and returns another <code>Stream</code>, therefore, it can be made into cascades or pipelines.</p>
<p>Using <code>Stream</code> is very intuitive:</p>
<figure class="highlight"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
</pre></td><td class="code"><pre>IntStream<span class="built_in">.</span><span class="keyword">iterate</span>(<span class="number">0</span>, i<span class="subst">-&gt;</span>i<span class="subst">+</span><span class="number">1</span>)
            <span class="built_in">.</span>filter(i<span class="subst">-&gt;</span>i<span class="subst">&gt;</span><span class="number">5</span>)
            <span class="built_in">.</span>limit(<span class="number">3</span>)
            <span class="built_in">.</span>forEach(System<span class="built_in">.</span>out<span class="tag">::println</span>);
<span class="comment">// Output:</span>
<span class="comment">// 6</span>
<span class="comment">// 7</span>
<span class="comment">// 8</span>
</pre></td></tr></table></figure>



<h3 id="3-_How_to_implement">3. How to implement</h3>
<h4 id="3-1_Using_Lambda_expressions_in_Java_8">3.1 Using Lambda expressions in Java 8</h4>
<ul>
<li>Shortest way</li>
</ul>
<figure class="highlight"><table><tr><td class="gutter"><pre>1
2
3
</pre></td><td class="code"><pre><span class="function"><span class="params">()</span> -&gt;</span> <span class="number">5</span> // Without input variable, <span class="keyword">return</span> a number
x<span class="function"> -&gt;</span> <span class="number">2</span> * x <span class="regexp">//</span> Take <span class="number">1</span> input variable, <span class="keyword">return</span> result <span class="keyword">of</span> multipling <span class="keyword">by</span> <span class="number">2</span>
<span class="function"><span class="params">( x, y )</span> -&gt;</span> x - y <span class="regexp">//</span> Take <span class="number">2</span> input variables, <span class="keyword">return</span> their substraction
</pre></td></tr></table></figure>

<ul>
<li>With type specification</li>
</ul>
<figure class="highlight"><table><tr><td class="gutter"><pre>1
2
</pre></td><td class="code"><pre><span class="function"><span class="params">(int x, int y)</span> -&gt;</span> x + y <span class="regexp">//</span> Take <span class="number">2</span> integers <span class="keyword">and</span> <span class="keyword">return</span> their sum
<span class="function"><span class="params">(String s)</span> -&gt;</span> System.out.println(s) <span class="regexp">//</span> Take <span class="number">1</span> String input <span class="keyword">and</span> <span class="keyword">return</span> nothing
</pre></td></tr></table></figure>

<ul>
<li>Using functional interface</li>
</ul>
<figure class="highlight"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
9
</pre></td><td class="code"><pre>@FunctionalInterface
<span class="keyword">public</span> <span class="keyword">interface</span> Discount {
    <span class="keyword">public</span> <span class="keyword">double</span> <span class="title">apply</span>(<span class="keyword">double</span> originalPrice);
}

<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span>(String[] args) {
    Discount discount = (x) -&gt; <span class="number">0.7</span> * x;
    System.<span class="keyword">out</span>.println(discount.apply(<span class="number">100</span>)); <span class="comment">// 70.0</span>
}
</pre></td></tr></table></figure>

<ul>
<li>Using Function type</li>
</ul>
<figure class="highlight"><table><tr><td class="gutter"><pre>1
</pre></td><td class="code"><pre>java.util.<span class="keyword">function</span>.<span class="keyword">Function</span>&lt;<span class="built_in">Double</span>, <span class="built_in">Double</span>&gt; discountFunction = (x) -&gt; <span class="number">0.6</span>*x;
</pre></td></tr></table></figure>

<h4 id="3-2_Using_Stream_APIs_in_Java_8">3.2 Using Stream APIs in Java 8</h4>
<p>For operating with <code>Stream</code> APIs, there are 3 kinds of methods:</p>
<ul>
<li><p><strong>Creating a stream</strong>: <code>Stream.iterate()</code>, <code>Stream.generate()</code>, <code>Stream.of()</code>, <code>IntStream.of()</code>, <code>Array.asList().stream()</code>, <code>list.stream()</code>, <code>Array.stream()</code>, <code>String.chars()</code></p>
</li>
<li><p><strong>Intermediate Operations</strong>: <code>peek()</code>, <code>map()</code>, <code>filter()</code>, <code>sorted()</code>, <code>limit()</code></p>
</li>
</ul>
<ul>
<li><strong>Terminal Operations</strong>: <code>reduce()</code>, <code>count()</code>, <code>forEach()</code>, <code>match()</code>, <code>findFirst()</code></li>
</ul>
<p>The first kind initially generate a stream from an existing <code>Collection</code> instance or from a pair of <code>seed</code> and <code>function</code>.</p>
<p>Intermediate operations accept lambda expression as arguments and return a new <code>Stream</code> with &#x201C;processed&#x201D; elements. It must be followed by a terminal operation, other wise, due to laziness feature, it will not be executed.</p>
<p>Terminal operations either doesn&#x2019;t return anything or return a new instance of <code>Collection</code>.</p>
<p>Example:</p>
<figure class="highlight"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
</pre></td><td class="code"><pre>IntStream<span class="built_in">.</span><span class="keyword">iterate</span>(<span class="number">0</span>, i<span class="subst">-&gt;</span>i<span class="subst">+</span><span class="number">1</span>)
            <span class="built_in">.</span>filter(i<span class="subst">-&gt;</span>i<span class="subst">&gt;</span><span class="number">5</span>)
            <span class="built_in">.</span>limit(<span class="number">3</span>)
            <span class="built_in">.</span>forEach(System<span class="built_in">.</span>out<span class="tag">::println</span>);
<span class="comment">// Output:</span>
<span class="comment">// 6</span>
<span class="comment">// 7</span>
<span class="comment">// 8</span>
</pre></td></tr></table></figure><br>Here, it creates an int stream by assigning an initial value with a lambda expression, which is executed to update the value due to lazy evaluation. Then the stream is passed to an intermediate operation <code>filter</code>, which filters out the values that doesn&#x2019;t return <code>True</code> in the predicate lambda expression. After that, it is passed to another intermediate operation <code>limit</code>, which stops the stream from generating new values once it receives 3 values. At the end, it uses a terminal operation <code>forEach</code> to do the follow up operation printing out each element it receives.<br><br>Note that primitive types should be carefully used in Java <code>Stream</code>.<br><br>For example[2]:<br><br><figure class="highlight"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
</pre></td><td class="code"><pre><span class="keyword">final</span> Integer[] integers = <span class="comment">{1, 2, 3}</span>;
<span class="keyword">final</span> int[]     ints     = <span class="comment">{1, 2, 3}</span>;

Stream.<span class="keyword">of</span>(integers).forEach(System.<span class="keyword">out</span>::println); <span class="comment">//That works just fine</span>
Stream.<span class="keyword">of</span>(ints).forEach(System.<span class="keyword">out</span>::println);     <span class="comment">//That doesn&apos;t</span>
IntStream.<span class="keyword">of</span>(ints).forEach(System.<span class="keyword">out</span>::println);  <span class="comment">//Have to use IntStream instead</span>
</pre></td></tr></table></figure>

<p>This is because in <code>Stream</code> package, Java APIs are defined using generic types <code>Stream&lt;T&gt;</code> which only accepts a Class type. Luckily, there&#x2019;re special APIs to specifically deal with primitive types: <code>IntStream</code>, <code>LongStream</code> and <code>DoubleStream</code>. But also be carefull using these primitive type streams. For example:</p>
<figure class="highlight"><table><tr><td class="gutter"><pre>1
2
3
4
5
</pre></td><td class="code"><pre>IntStream<span class="preprocessor">.range</span>(<span class="number">0</span>, <span class="number">10</span>)<span class="preprocessor">.collect</span>(Collectors<span class="preprocessor">.toList</span>())<span class="comment">;</span>
// Cannot compile...

IntStream<span class="preprocessor">.range</span>(<span class="number">0</span>, <span class="number">10</span>)<span class="preprocessor">.boxed</span>()<span class="preprocessor">.collect</span>(Collectors<span class="preprocessor">.toList</span>())<span class="comment">;</span>
// Works!
</pre></td></tr></table></figure>

<p>Primitive types must be boxed to Objects in order to follow up by a <code>collect</code> operation.</p>
<h4 id="3-3_Comparing_with_Scheme">3.3 Comparing with Scheme</h4>
<ul>
<li><code>(car list)</code>, get the head of a list:</li>
</ul>
<figure class="highlight"><table><tr><td class="gutter"><pre>1
2
3
4
</pre></td><td class="code"><pre>List&lt;Integer&gt; list = Arrays<span class="preprocessor">.asList</span>(<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)<span class="comment">;</span>

Integer car = list<span class="preprocessor">.stream</span>()<span class="preprocessor">.findFirst</span>()<span class="preprocessor">.get</span>()<span class="comment">;</span>
System<span class="preprocessor">.out</span><span class="preprocessor">.println</span>(<span class="string">&quot;Car: &quot;</span> + car)<span class="comment">; // Car: 0</span>
</pre></td></tr></table></figure>

<ul>
<li><code>(cdr list)</code>, get the rest of a list:</li>
</ul>
<figure class="highlight"><table><tr><td class="gutter"><pre>1
2
</pre></td><td class="code"><pre>List&lt;Integer&gt; cdr = list<span class="preprocessor">.stream</span>()<span class="preprocessor">.skip</span>(<span class="number">1</span>)<span class="preprocessor">.collect</span>(Collectors<span class="preprocessor">.toList</span>())<span class="comment">;</span>
System<span class="preprocessor">.out</span><span class="preprocessor">.println</span>(<span class="string">&quot;Cdr: &quot;</span> + cdr)<span class="comment">; // Cdr: [1, 2, 3, 4]</span>
</pre></td></tr></table></figure>

<ul>
<li><code>(stream-car stream)</code>, get the first element of a stream:</li>
</ul>
<figure class="highlight"><table><tr><td class="gutter"><pre>1
2
</pre></td><td class="code"><pre>Integer stream_car = list<span class="preprocessor">.stream</span>()<span class="preprocessor">.findFirst</span>()<span class="preprocessor">.get</span>()<span class="comment">;</span>
System<span class="preprocessor">.out</span><span class="preprocessor">.println</span>(stream_car)<span class="comment">; // 0</span>
</pre></td></tr></table></figure>

<ul>
<li><code>(stream-cdr stream)</code>, get the rest of of the stream:</li>
</ul>
<figure class="highlight"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
7
</pre></td><td class="code"><pre>Stream s = <span class="keyword">list</span>.stream();
s.<span class="keyword">forEach</span>(System.out::<span class="keyword">print</span>); <span class="comment">// 01234</span>
Stream stream_cdr1 = s.skip(<span class="number">1</span>); <span class="comment">// Error: stream has already been operated upon or closed</span>

Stream s2 = <span class="keyword">list</span>.stream();
Stream stream_cdr2 = s2.skip(<span class="number">1</span>); <span class="comment">// Didn&apos;t end with a terminal operation, skip() is not computed</span>
stream_cdr2.<span class="keyword">forEach</span>(System.out::<span class="keyword">print</span>); <span class="comment">// 1234</span>
</pre></td></tr></table></figure>

<ul>
<li><code>(cons &apos;a &apos;(b c d))</code>, construct a list with a new head attached:</li>
</ul>
<figure class="highlight"><table><tr><td class="gutter"><pre>1
2
3
</pre></td><td class="code"><pre>List&lt;String&gt; list = Arrays<span class="preprocessor">.asList</span>(<span class="string">&quot;b&quot;</span>, <span class="string">&quot;c&quot;</span>, <span class="string">&quot;d&quot;</span>)<span class="comment">;</span>
List&lt;String&gt; cons_list = Stream<span class="preprocessor">.concat</span>(Stream<span class="preprocessor">.of</span>(<span class="string">&quot;a&quot;</span>), list<span class="preprocessor">.stream</span>())<span class="preprocessor">.collect</span>(Collectors<span class="preprocessor">.toList</span>())<span class="comment">;</span>
System<span class="preprocessor">.out</span><span class="preprocessor">.println</span>(cons_list)<span class="comment">; // [a, b, c, d]</span>
</pre></td></tr></table></figure>

<ul>
<li><code>(cons-stream n stream)</code>, construct a stream with a new head attached:</li>
</ul>
<figure class="highlight"><table><tr><td class="gutter"><pre>1
2
</pre></td><td class="code"><pre>Stream cons_stream = Stream<span class="preprocessor">.concat</span>(Stream<span class="preprocessor">.of</span>(n), list<span class="preprocessor">.stream</span>())<span class="comment">;</span>
cons_stream<span class="preprocessor">.forEach</span>(System<span class="preprocessor">.out</span>::print)<span class="comment">; // abcd</span>
</pre></td></tr></table></figure>



<h3 id="4-_Hightlights">4. Hightlights</h3>
<ul>
<li>Simplified syntax</li>
</ul>
<p>Using <code>Lambda</code> and <code>Stream</code> APIs can reduce the complexity of code. This is pretty obvious.</p>
<ul>
<li>Easy to parallelize</li>
</ul>
<p>By using <code>Stream</code> APIs, one can easily parallelize the process simply by changing <code>stream()</code> to <code>parallelStream()</code>. For example:</p>
<figure class="highlight"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
</pre></td><td class="code"><pre>// Generate a list with <span class="number">10000000</span> elements
List&lt;Integer&gt; list = IntStream<span class="preprocessor">.range</span>(<span class="number">0</span>, <span class="number">10000000</span>)
                            <span class="preprocessor">.boxed</span>()<span class="preprocessor">.collect</span>(Collectors<span class="preprocessor">.toList</span>())<span class="comment">;</span>

// Sequencial
long startTime = System<span class="preprocessor">.currentTimeMillis</span>()<span class="comment">;</span>
List&lt;Integer&gt; evenList = list<span class="preprocessor">.stream</span>()
                            <span class="preprocessor">.filter</span>(i-&gt;i%<span class="number">2</span>==<span class="number">0</span>)<span class="preprocessor">.collect</span>(Collectors<span class="preprocessor">.toList</span>())<span class="comment">;</span>
long estimatedTime = System<span class="preprocessor">.currentTimeMillis</span>() - startTime<span class="comment">;</span>

// Parallelized
long startTime2 = System<span class="preprocessor">.currentTimeMillis</span>()<span class="comment">;</span>
List&lt;Integer&gt; oddList  = list<span class="preprocessor">.parallelStream</span>()
                            <span class="preprocessor">.filter</span>(i-&gt;i%<span class="number">2</span>!=<span class="number">0</span>)<span class="preprocessor">.collect</span>(Collectors<span class="preprocessor">.toList</span>())<span class="comment">;</span>
long estimatedTime2 = System<span class="preprocessor">.currentTimeMillis</span>() - startTime2<span class="comment">;</span>
System<span class="preprocessor">.out</span><span class="preprocessor">.println</span>(<span class="string">&quot;Time1: &quot;</span> + estimatedTime + <span class="string">&quot; Time2: &quot;</span> + estimatedTime2)<span class="comment">;</span>

// Output:
// Time1: <span class="number">1987</span> Time2: <span class="number">1029</span>
</pre></td></tr></table></figure>

<p>If the process doesn&#x2019;t involve any synchronized variable or operation (here the collect operation is working on one list, therefore need to be locked and unlocked), the improvement would be much significant. The parallel processes is managed under the hood, which means one cannot assign explicitly how many threads to run. But in this way the code has more &#x201C;cross platform&#x201D; benifits, so that one don&#x2019;t have change the parallel settings since everything is handled automatically.</p>
<ul>
<li>Laziness</li>
</ul>
<p>Harnessing the benifit of lazy evaluation will help to improve the efficiency of the program. One can define a lot of <code>Stream</code> but as long as it&#x2019;s not followed by a terminal operation, it&#x2019;s not executed. Furthermore, due to laziness feature of <code>Stream</code>, datas are generated &#x201C;on the fly&#x201D; instead of creating a whole list of elements and then iterate through them, not to mention <code>List</code> cannot be infinent, which a <code>Stream</code> can achieve easily.</p>
<h3 id="5-_Limitations">5. Limitations</h3>
<ul>
<li>Functional interface limitation</li>
</ul>
<p>As mentioned before, lambdas in Java 8 are &#x201C;Syntactic Sugars&#x201D; to map &#x201C;arrow style&#x201D; to anonymous inner class definations. All lambdas must has a functional interface with matching signature. This is due to Java&#x2019;s static type limitation. Although <code>java.util.function</code> package has provided a variaty of functional interfaces with generic types and compiler can use type inference to find the matching interface, one has to define his own interfaces if they&#x2019;re not provided, for example, a lambda with 3 input arguments.</p>
<ul>
<li>External variable referencing</li>
</ul>
<p>Java does not allow changing the value or reference of a outside variable inside an anonymous inner class, and all variables that declared outside the anonymous class are inexplictly restrained to <code>final</code> inside. So this regulation also applies to lambdas.</p>
<p>For example:</p>
<figure class="highlight"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
9
10
11
</pre></td><td class="code"><pre>int a <span class="subst">=</span> <span class="number">1</span>;
                
<span class="keyword">Thread</span> t1 <span class="subst">=</span> <span class="literal">new</span> <span class="keyword">Thread</span>(<span class="literal">new</span> Runnable() {     
    <span class="keyword">public</span> <span class="literal">void</span> run() {
        a <span class="subst">=</span> <span class="number">2</span>; <span class="comment">// This cannot pass the compilation, because `a` is declared </span>
        <span class="comment">// inexplictly `final` inside, therefore can&apos;t be changed.</span>
    }
});

<span class="keyword">Thread</span> t2 <span class="subst">=</span> <span class="literal">new</span> <span class="keyword">Thread</span>(
        ()<span class="subst">-&gt;</span>{a <span class="subst">=</span> <span class="number">2</span>;} ); <span class="comment">// This cannot pass either.</span>
</pre></td></tr></table></figure>

<h3 id="6-_References">6. References</h3>
<p>1: <a href="http://www.ibm.com/developerworks/java/library/j-java8lambdas/index.html" target="_blank" rel="external">http://www.ibm.com/developerworks/java/library/j-java8lambdas/index.html</a></p>
<p>2: <a href="http://stackoverflow.com/questions/23007422/using-streams-with-primitives-data-types-and-corresponding-wrappers" target="_blank" rel="external">http://stackoverflow.com/questions/23007422/using-streams-with-primitives-data-types-and-corresponding-wrappers</a></p>
]]></content>
    
    
      <category term="java" scheme="http://hugofeng.info/tags/java/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Fast Copying Timestamp on Mac]]></title>
    <link href="http://hugofeng.info/2014/08/18/Auto_Timestamp/"/>
    <id>http://hugofeng.info/2014/08/18/Auto_Timestamp/</id>
    <published>2014-08-18T16:56:19.000Z</published>
    <updated>2014-08-21T10:57:01.000Z</updated>
    <content type="html"><![CDATA[<p>When I&#x2019;m using Googe Docs to write logs for my lab work, it&#x2019;ll be much more convenient to insert timestamp and I&#x2019;m surprised that they don&#x2019;t have this feature (Microsoft Word has it). </p>
<p>But if you are using Mac OSX, this would be very easy to achive. Since there&#x2019;s a pretty good command line tool called <code>date</code> that can print timestamp in multiple formats. The thing we need to do is to copy the standard output to the clipboard. </p>
<p>There are 2 ways to do this:</p>
<ul>
<li>Using <code>Alfred</code>. <code>Alfred</code> is a tool to quick launch applications and it has a feature of quick run customized shell scripts. However, this feature is together with its <code>Power Pack</code>, which is limited to paid users. </li>
<li>Using <code>automator</code>. <code>Automator</code> is a tool provided by Mac OSX that can pack scripts or recorded actions into an application or service. Here we choose this method.</li>
</ul>
<p>Step:</p>
<ol>
<li>launch <code>Automator</code> in <code>Launcher</code>. </li>
<li>In menu bar, choose <code>File</code> -&gt; <code>New</code>, then choose <code>Application</code>.</li>
<li>In the search bar search for <code>Run Shell Script</code>.</li>
<li><p>In the right window, put in the following script:</p>
<pre><code> <span class="built_in">date</span> <span class="subst">|</span>tr <span class="attribute">-d</span> <span class="string">&apos;\n&apos;</span><span class="subst">|</span> pbcopy
</code></pre></li>
<li><p>In menu bar, choose <code>File</code> -&gt; <code>Save</code> to save your application, better to save it to <code>/Applications</code> for <code>Spotlight</code> to index. Here we name it as <code>TimeStamp</code>.</p>
</li>
</ol>
<p>Voila! Now when you are using Google Docs or any other text editor, use <code>Spotlight</code> or <code>Alfred</code> to launch the application we just created, and press key <code>Command</code> + <code>V</code> to paste the timestamp to the document. You can also download my application here.<a href="/files/TimeStamp.zip">Download my TimeStamp.app</a></p>
<p>Explaining the script:</p>
<p>First <code>date</code> command will execute the <code>date</code> program and its output will be redirected to <code>tr</code> program, which with argument <code>-d &apos;\n&apos;</code>, will delete the last &#x201C;endline&#x201D; character. At last, the result will be past to <code>pbcopy</code> program to add it to the clipboard.</p>
<p>Further more, we can also add arguments to <code>date</code> to customize the format of the timestamp. Here is a nice article about this. <a href="http://www.cyberciti.biz/faq/linux-unix-formatting-dates-for-display/" target="_blank" rel="external">Link.</a> For myself, I use this <code>date +&quot;%d/%m/%y %a %H:%M&quot;</code>, which print something like this: <code>18/08/14 Mon 20:25</code> .</p>
]]></content>
    
    
      <category term="Mac shell" scheme="http://hugofeng.info/tags/Mac-shell/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[An implementation of Neural Network with Back Propagation in MATLAB]]></title>
    <link href="http://hugofeng.info/2014/06/05/bp_ann_in_matlab/"/>
    <id>http://hugofeng.info/2014/06/05/bp_ann_in_matlab/</id>
    <published>2014-06-05T20:45:14.000Z</published>
    <updated>2014-06-07T20:13:03.000Z</updated>
    <content type="html"><![CDATA[<h3 id="Test_the_program_with_a_data_set">Test the program with a data set</h3>
<ul>
<li><strong>Network attributes</strong>: 1 hidden layer with 3 neurons, initial biases and weights are set to random from -1 to 1, with 0.5 learning rate</li>
<li><strong>Target</strong>: XOR(x1, x2)</li>
<li><strong>Training</strong> set: [x1, x2] = [0 0; 0 1; 1 0; 1 1], y = [0; 1; 1; 0]</li>
<li><strong>Test set</strong>: (Same as training set),   error = (sum(errors .^ 2))^0.5</li>
</ul>
<h4 id="Output">Output</h4>
<p><img src="/img/bp1.png" alt="Standard diviation of errors with iterations"><br><img src="/img/bp2.png" alt="Program output"><br><img src="/img/bp3.png" alt="Output of trained Neural Network"></p>
<a id="more"></a>

<h3 id="Analyze">Analyze</h3>
<p>Since neural network with activation function Sigmoid, is a highly non-linear system, the formation of the network can be depended on the initial value of weights and biases, and its non-linear feature may grow with the number of layers as well as number of neurons. So sometimes, the following network formation can also be trained from the same training set and settings.</p>
<p><img src="/img/bp4.png" alt="Another trained Network1"><br><img src="/img/bp5.png" alt="Another trained Network2"></p>
<h3 id="Source_Code_in_MATLAB">Source Code in MATLAB</h3>
<figure class="highlight MATLAB"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
</pre></td><td class="code"><pre><span class="comment">%% BPANN: Artificial Neural Network with Back Propagation</span>
<span class="comment">%% Author: Xuyang Feng</span>
<span class="function"><span class="keyword">function</span>  <span class="title">BPANN</span><span class="params">()</span></span>

    <span class="comment">%---Set training parameters</span>
    iterations = <span class="number">5000</span>;
    errorThreshhold = <span class="number">0.1</span>;
    learningRate = <span class="number">0.5</span>;
    <span class="comment">%---Set hidden layer type, for example: [4, 3, 2]</span>
    hiddenNeurons = <span class="matrix">[<span class="number">3</span> <span class="number">2</span>]</span>;

    
    <span class="comment">%---&apos;Xor&apos; training data</span>
    trainInp = <span class="matrix">[<span class="number">0</span> <span class="number">0</span>; <span class="number">0</span> <span class="number">1</span>; <span class="number">1</span> <span class="number">0</span>; <span class="number">1</span> <span class="number">1</span>]</span>;
    trainOut = <span class="matrix">[<span class="number">0</span>; <span class="number">1</span>; <span class="number">1</span>; <span class="number">0</span>]</span>;
    testInp = trainInp;
    testRealOut = trainOut;
    

    <span class="comment">% %---&apos;And&apos; training data</span>
    <span class="comment">% trainInp = [1 1; 1 0; 0 1; 0 0];</span>
    <span class="comment">% trainOut = [1; 0; 0; 0];</span>
    <span class="comment">% testInp = trainInp;</span>
    <span class="comment">% testRealOut = trainOut;</span>

    assert(<span class="built_in">size</span>(trainInp,<span class="number">1</span>)==<span class="built_in">size</span>(trainOut, <span class="number">1</span>),...
        <span class="string">&apos;Counted different sets of input and output.&apos;</span>);

    <span class="comment">%---Initialize Network attributes</span>
    inArgc = <span class="built_in">size</span>(trainInp, <span class="number">2</span>);
    outArgc = <span class="built_in">size</span>(trainOut, <span class="number">2</span>);
    trainsetCount = <span class="built_in">size</span>(trainInp, <span class="number">1</span>);
    
    <span class="comment">%---Add output layer</span>
    layerOfNeurons = <span class="matrix">[hiddenNeurons, outArgc]</span>;
    layerCount = <span class="built_in">size</span>(layerOfNeurons, <span class="number">2</span>);
    
    <span class="comment">%---Weight and bias random range</span>
    e = <span class="number">1</span>;
    b = -e;

    <span class="comment">%---Set initial random weights</span>
    weightCell = cell(<span class="number">1</span>, layerCount);
    <span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:layerCount
        <span class="keyword">if</span> <span class="built_in">i</span> == <span class="number">1</span>
            weightCell<span class="cell">{<span class="number">1</span>}</span> = unifrnd(b, e, inArgc,layerOfNeurons(<span class="number">1</span>));
        <span class="keyword">else</span>
            weightCell<span class="cell">{i}</span> = unifrnd(b, e, layerOfNeurons(<span class="built_in">i</span>-<span class="number">1</span>),layerOfNeurons(<span class="built_in">i</span>));
        <span class="keyword">end</span>
    <span class="keyword">end</span>

    <span class="comment">%---Set initial biases</span>
    biasCell = cell(<span class="number">1</span>, layerCount);
    <span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:layerCount
        biasCell<span class="cell">{i}</span> = unifrnd(b, e, <span class="number">1</span>, layerOfNeurons(<span class="built_in">i</span>));
    <span class="keyword">end</span>


    <span class="comment">%----------------------</span>
    <span class="comment">%---Begin training</span>
    <span class="comment">%----------------------</span>
    <span class="keyword">for</span> iter = <span class="number">1</span>:iterations
        <span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:trainsetCount
            <span class="comment">% choice = randi([1 trainsetCount]);</span>
            choice = <span class="built_in">i</span>;
            sampleIn = trainInp(choice, :);
            sampleTarget = trainOut(choice, :);

            <span class="matrix">[realOutput, layerOutputCells]</span> = ForwardNetwork(sampleIn, layerOfNeurons, weightCell, biasCell);
            <span class="matrix">[weightCell, biasCell]</span> = BackPropagate(learningRate, sampleIn, realOutput, sampleTarget, layerOfNeurons, ...
                weightCell, biasCell, layerOutputCells);
        <span class="keyword">end</span>

        <span class="comment">%plot overall network error at end of each iteration</span>
        error = <span class="built_in">zeros</span>(trainsetCount, outArgc);
        <span class="keyword">for</span> t = <span class="number">1</span>:trainsetCount
            <span class="matrix">[predict, layeroutput]</span> = ForwardNetwork(trainInp(t, :), layerOfNeurons, weightCell, biasCell);
            p(t) = predict;
            error(t, : ) = predict - trainOut(t, :);
        <span class="keyword">end</span>

        err(iter) = (sum(<span class="transposed_variable">error.</span>^<span class="number">2</span>)/trainsetCount)^<span class="number">0.5</span>;
        figure(<span class="number">1</span>);
        plot(err);

        <span class="comment">%---Stop if reach error threshold</span>
        <span class="keyword">if</span> err(iter) &lt; errorThreshhold
            <span class="keyword">break</span>;
        <span class="keyword">end</span>
    <span class="keyword">end</span>
    
    <span class="comment">%--Test the trained network with a test set</span>
    testsetCount = <span class="built_in">size</span>(testInp, <span class="number">1</span>);
    error = <span class="built_in">zeros</span>(testsetCount, outArgc);
    <span class="keyword">for</span> t = <span class="number">1</span>:testsetCount
        <span class="matrix">[predict, layeroutput]</span> = ForwardNetwork(testInp(t, :), layerOfNeurons, weightCell, biasCell);
        p(t) = predict;
        error(t, : ) = predict - testRealOut(t, :);
    <span class="keyword">end</span>

    <span class="comment">%---Print predictions</span>
    fprintf(<span class="string">&apos;Ended with %d iterations.\n&apos;</span>, iter);
    a = testInp;
    b = testRealOut;
    c = <span class="transposed_variable">p&apos;</span>;
    x1_x2_act_pred_err = <span class="matrix">[a b c c-b]</span>

    <span class="comment">%---Plot Surface of network predictions</span>
    testInpx1 = <span class="matrix">[-<span class="number">1</span>:<span class="number">0.1</span>:<span class="number">1</span>]</span>;
    testInpx2 = <span class="matrix">[-<span class="number">1</span>:<span class="number">0.1</span>:<span class="number">1</span>]</span>;
    <span class="matrix">[X1, X2]</span> = <span class="built_in">meshgrid</span>(testInpx1, testInpx2);
    testOutRows = <span class="built_in">size</span>(X1, <span class="number">1</span>);
    testOutCols = <span class="built_in">size</span>(X1, <span class="number">2</span>);
    testOut = <span class="built_in">zeros</span>(testOutRows, testOutCols);
    <span class="keyword">for</span> row = <span class="matrix">[<span class="number">1</span>:testOutRows]</span>
        <span class="keyword">for</span> col = <span class="matrix">[<span class="number">1</span>:testOutCols]</span>
            test = <span class="matrix">[X1(row, col), X2(row, col)]</span>;
            <span class="matrix">[out, l]</span> = ForwardNetwork(test, layerOfNeurons, weightCell, biasCell);
            testOut(row, col) = out;
        <span class="keyword">end</span>
    <span class="keyword">end</span>
    figure(<span class="number">2</span>);
    surf(X1, X2, testOut);

<span class="keyword">end</span>

<span class="comment">%% BackPropagate: Backpropagate the output through the network and adjust weights and biases</span>
<span class="function"><span class="keyword">function</span> <span class="params">[weightCell, biasCell]</span> = <span class="title">BackPropagate</span><span class="params">(rate, in, realOutput, sampleTarget, layer, weightCell, biasCell, layerOutputCells)</span></span>
    layerCount = <span class="built_in">size</span>(layer, <span class="number">2</span>);
    delta = cell(<span class="number">1</span>, layerCount);
    D_weight = cell(<span class="number">1</span>, layerCount);
    D_bias = cell(<span class="number">1</span>, layerCount);
    <span class="comment">%---From Output layer, it has different formula</span>
    output = layerOutputCells<span class="cell">{layerCount}</span>;
    delta<span class="cell">{layerCount}</span> = output .* (<span class="number">1</span>-output) .* (sampleTarget - output);
    preoutput = layerOutputCells<span class="cell">{layerCount-<span class="number">1</span>}</span>;
    D_weight<span class="cell">{layerCount}</span> = rate .* <span class="transposed_variable">preoutput&apos;</span> * delta<span class="cell">{layerCount}</span>;
    D_bias<span class="cell">{layerCount}</span> = rate .* delta<span class="cell">{layerCount}</span>;

    <span class="comment">%---Back propagate for Hidden layers</span>
    <span class="keyword">for</span> layerIndex = layerCount-<span class="number">1</span>:-<span class="number">1</span>:<span class="number">1</span>
        output = layerOutputCells<span class="cell">{layerIndex}</span>;
        <span class="keyword">if</span> layerIndex == <span class="number">1</span>
            preoutput = in;
        <span class="keyword">else</span>
            preoutput = layerOutputCells<span class="cell">{layerIndex-<span class="number">1</span>}</span>;
        <span class="keyword">end</span>

        weight = weightCell<span class="cell">{layerIndex+<span class="number">1</span>}</span>;
        sumup = (weight * delta<span class="cell">{layerIndex+<span class="number">1</span>}&apos;</span>)<span class="string">&apos;;
        delta{layerIndex} = output .* (1 - output) .* sumup;

        D_weight{layerIndex} = rate .* preoutput&apos;</span> * delta<span class="cell">{layerIndex}</span>;
        D_bias<span class="cell">{layerIndex}</span> = rate .* delta<span class="cell">{layerIndex}</span>;
    <span class="keyword">end</span>
    <span class="comment">%---Update weightCell and biasCell</span>
    <span class="keyword">for</span> layerIndex = <span class="number">1</span>:layerCount
        weightCell<span class="cell">{layerIndex}</span> = weightCell<span class="cell">{layerIndex}</span> + D_weight<span class="cell">{layerIndex}</span>;
        biasCell<span class="cell">{layerIndex}</span> = biasCell<span class="cell">{layerIndex}</span> + D_bias<span class="cell">{layerIndex}</span>;
    <span class="keyword">end</span>


<span class="keyword">end</span>


<span class="comment">%% ForwardNetwork: Compute feed forward neural network, Return the output and output of each neuron in each layer</span>
<span class="function"><span class="keyword">function</span> <span class="params">[realOutput, layerOutputCells]</span> = <span class="title">ForwardNetwork</span><span class="params">(in, layer, weightCell, biasCell)</span></span>
    layerCount = <span class="built_in">size</span>(layer, <span class="number">2</span>);
    layerOutputCells = cell(<span class="number">1</span>, layerCount);

    out = in;
    <span class="keyword">for</span> layerIndex = <span class="number">1</span>:layerCount
        X = out;
        bias = biasCell<span class="cell">{layerIndex}</span>;
        out = Sigmoid(X * weightCell<span class="cell">{layerIndex}</span> + bias);
        layerOutputCells<span class="cell">{layerIndex}</span> = out;
    <span class="keyword">end</span>
    realOutput = out;    
<span class="keyword">end</span>
</pre></td></tr></table></figure>
]]></content>
    <summary type="html"><![CDATA[<h3 id="Test_the_program_with_a_data_set">Test the program with a data set</h3>
<ul>
<li><strong>Network attributes</strong>: 1 hidden layer with 3 neurons, initial biases and weights are set to random from -1 to 1, with 0.5 learning rate</li>
<li><strong>Target</strong>: XOR(x1, x2)</li>
<li><strong>Training</strong> set: [x1, x2] = [0 0; 0 1; 1 0; 1 1], y = [0; 1; 1; 0]</li>
<li><strong>Test set</strong>: (Same as training set),   error = (sum(errors .^ 2))^0.5</li>
</ul>
<h4 id="Output">Output</h4>
<p><img src="/img/bp1.png" alt="Standard diviation of errors with iterations"><br><img src="/img/bp2.png" alt="Program output"><br><img src="/img/bp3.png" alt="Output of trained Neural Network"></p>
]]></summary>
    
      <category term="ML" scheme="http://hugofeng.info/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Floating point operations on ARM processor]]></title>
    <link href="http://hugofeng.info/2014/04/25/float-operations-on-arm/"/>
    <id>http://hugofeng.info/2014/04/25/float-operations-on-arm/</id>
    <published>2014-04-25T19:32:34.000Z</published>
    <updated>2014-05-06T23:15:42.000Z</updated>
    <content type="html"><![CDATA[<p>This article is a brief introduction of different implementations of floating point operation on an ARM processor.</p>
<h3 id="1-_Early_ages">1. Early ages</h3>
<p>There&#x2019;s no coprocessor for floating point operations on ARM, those operations are done by CPU, means of which is called <code>Float Math Emulation</code>. Normally each float point operation costs thousands of cycles, which is very inefficient. </p>
<h3 id="2-_Soft-float">2. Soft-float</h3>
<p>This is for those situations where the CPU does not contain a <code>floating point unit(FPU)</code>, which is a built-in coprocessor on CPU to deal with floating point operations. </p>
<p>There are 2 different ways to do floating point operations without FPU: </p>
<ol>
<li><p>The kernel will trap all the opcodes related to the FPU and then perform the calculations itself [1]. This is called <code>NWFPE</code> (NetWinder Floating Point Emulator), however, this has now been removed from ARM kernel [2]. As I mentioned before, this is the old fashioned way of implementation, which involves raising of invalid instruction exceptions each time the situation occurs [3]. And this can be very expensive for computation. </p>
</li>
<li><p>All floating point operations are translated to specific inline function calls by compiler. This is done by passing flag <code>-mfloat-abi=soft</code> when using <code>gcc</code>, and gcc will use built in software(library) to emulate.</p>
</li>
</ol>
<h3 id="3-_Hard-float">3. Hard-float</h3>
<p>Nowadays, many chips has hardware <code>FPU</code> support to accelerate fp operations, for ARM familis, this unit is often called <code>VFP</code> (Vector Floating-Point coprocessor). VFP is a fully IEEE-754 compatible floating point unit. But later a much more powerful NEON Advanced SIMD unit was introduced and is suggested instead of VFP by Architecture Reference Manual.</p>
<a id="more"></a>

<h4 id="3-1_VFP">3.1 VFP</h4>
<p>One of the nice feature VFP provides is that it supports single and double-precision arithmetic on vector-vector, vector-scalar, and scalar-scalar data sets where vectors can consist of up to 8 single-precision, or 4 double-precision elements [4]. The &#x201C;vector mode&#x201D; instructions of VFP is actually sequential, so the speed up is very limited [8]. There&#x2019;s one thing worth mentioning, that the &#x201C;vector mode&#x201D; of VFP is actually deprecated, replaced shortly after its introduce, with the much more powerful NEON Advanced SIMD unit [9]. </p>
<p>To use VFP, one need to pass flag <code>-mfloat-abi=softfp</code> or <code>-mfloat-abi=hard</code> to gcc when compiling.</p>
<h5 id="3-1-1_Difference_between_softfp_and_hard_mode">3.1.1 Difference between <code>softfp</code> and <code>hard</code> mode</h5>
<ul>
<li><p>For <code>softfp</code> mode:<br>Using general integer registers to pass values (like the <code>soft</code> mode). It also support linking to <code>soft</code> mode compiled binaris. </p>
</li>
<li><p>For <code>hard</code> mode:<br>Using floating point registers on FPU to pass values. Doesn&#x2019;t support linking to <code>soft</code> mode binaris. All codes must be compiled in <code>hard</code> mode. It saves up to 20 cycles when calling a function with fp arguments, therefore faster than <code>softfp</code> mode [6].</p>
</li>
</ul>
<p>Nowadays, ARM Linux is set to <code>hard</code> mode by default [3] while Debian Armel set <code>softfp</code> as default [7]. And these two modes are not compatible in one application due to different value passing conventions. </p>
<p>Besides, <code>-msoft-float</code> equals to <code>-mfloat-abi=soft</code>, so as <code>-mhard-float</code> to <code>-mfloat-abi=hard</code> [8].</p>
<h4 id="3-2_New_era,_with_Neon">3.2 New era, with Neon</h4>
<p>The Advanced SIMD extension (aka NEON or &#x201C;MPE&#x201D; Media Processing Engine) is a combined 64- and 128-bit SIMD instruction set that provides standardized acceleration for media and signal processing applications. NEON is included in all Cortex-A8 devices but is optional in Cortex-A9 devices [9]. </p>
<p>Neon shares the same floating point registers with VFP, and thanks to floating point pipeline technology that Neon supports, it is a lot faster than VFP, but the draw back is that the NEON floating point pipeline is not entirely IEEE-754 compliant[10]. </p>
<p>Following is quoted from Peter on <a href="http://stackoverflow.com/questions/4097034/arm-cortex-a8-whats-the-difference-between-vfp-and-neon" target="_blank" rel="external">StackOverflow</a>. He&#x2019;s got a good introduction on this.</p>
<blockquote>
<p>Because it is not IEEE-754 compliant, a compiler cannot generate these instructions unless you tell the compiler that you are not interested in full compliance. This can be done in several ways.</p>
<ol>
<li><p>Using an intrinsic function to force NEON usage, for example see the <a href="http://gcc.gnu.org/onlinedocs/gcc/ARM-NEON-Intrinsics.html" target="_blank" rel="external">GCC Neon Intrinsic Function List</a>.</p>
</li>
<li><p>Ask the compiler, very nicely. Even newer GCC versions with <code>-mfpu=neon</code> will not generate floating point NEON instructions unless you also specify <code>-funsafe-math-optimizations</code>.</p>
</li>
</ol>
</blockquote>
<h3 id="4-_Conclusion">4. Conclusion</h3>
<p>ARM chips is not specifically designed for floating point operations, but with the help of coprocessors, the speed of these expensive operations can be greatly improved.</p>
<h3 id="5-_Reference">5. Reference</h3>
<ul>
<li>[1] <a href="http://www.nslu2-linux.org/wiki/FAQ/SoftHardFloatCompiler" target="_blank" rel="external">http://www.nslu2-linux.org/wiki/FAQ/SoftHardFloatCompiler</a></li>
<li>[2] <a href="http://lwn.net/Articles/546840/" target="_blank" rel="external">http://lwn.net/Articles/546840/</a></li>
<li>[3] <a href="http://linux-7110.sourceforge.net/howtos/netbook_new/x1114.htm" target="_blank" rel="external">http://linux-7110.sourceforge.net/howtos/netbook_new/x1114.htm</a></li>
<li>[4] <a href="http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.ddi0301h/Cegdejjh.html" target="_blank" rel="external">http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.ddi0301h/Cegdejjh.html</a></li>
<li>[5] <a href="http://www.gurucoding.com/en/rpi_cross_compiler/diff_hardfp_softfp.php" target="_blank" rel="external">http://www.gurucoding.com/en/rpi_cross_compiler/diff_hardfp_softfp.php</a></li>
<li>[6] <a href="https://community.freescale.com/thread/219966" target="_blank" rel="external">https://community.freescale.com/thread/219966</a></li>
<li>[7] <a href="https://wiki.debian.org/ArmEabiPort" target="_blank" rel="external">https://wiki.debian.org/ArmEabiPort</a></li>
<li>[8] <a href="http://gcc.gnu.org/onlinedocs/gcc-4.1.1/gcc/ARM-Options.html#ARM-Options" target="_blank" rel="external">http://gcc.gnu.org/onlinedocs/gcc-4.1.1/gcc/ARM-Options.html#ARM-Options</a></li>
<li>[9] <a href="http://en.wikipedia.org/wiki/ARM_architecture" target="_blank" rel="external">http://en.wikipedia.org/wiki/ARM_architecture</a></li>
<li>[10] <a href="http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.dui0204h/Bcfhfbga.html" target="_blank" rel="external">http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.dui0204h/Bcfhfbga.html</a></li>
</ul>
]]></content>
    <summary type="html"><![CDATA[<p>This article is a brief introduction of different implementations of floating point operation on an ARM processor.</p>
<h3 id="1-_Early_ages">1. Early ages</h3>
<p>There&#x2019;s no coprocessor for floating point operations on ARM, those operations are done by CPU, means of which is called <code>Float Math Emulation</code>. Normally each float point operation costs thousands of cycles, which is very inefficient. </p>
<h3 id="2-_Soft-float">2. Soft-float</h3>
<p>This is for those situations where the CPU does not contain a <code>floating point unit(FPU)</code>, which is a built-in coprocessor on CPU to deal with floating point operations. </p>
<p>There are 2 different ways to do floating point operations without FPU: </p>
<ol>
<li><p>The kernel will trap all the opcodes related to the FPU and then perform the calculations itself [1]. This is called <code>NWFPE</code> (NetWinder Floating Point Emulator), however, this has now been removed from ARM kernel [2]. As I mentioned before, this is the old fashioned way of implementation, which involves raising of invalid instruction exceptions each time the situation occurs [3]. And this can be very expensive for computation. </p>
</li>
<li><p>All floating point operations are translated to specific inline function calls by compiler. This is done by passing flag <code>-mfloat-abi=soft</code> when using <code>gcc</code>, and gcc will use built in software(library) to emulate.</p>
</li>
</ol>
<h3 id="3-_Hard-float">3. Hard-float</h3>
<p>Nowadays, many chips has hardware <code>FPU</code> support to accelerate fp operations, for ARM familis, this unit is often called <code>VFP</code> (Vector Floating-Point coprocessor). VFP is a fully IEEE-754 compatible floating point unit. But later a much more powerful NEON Advanced SIMD unit was introduced and is suggested instead of VFP by Architecture Reference Manual.</p>
]]></summary>
    
      <category term="embedded" scheme="http://hugofeng.info/tags/embedded/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Shell Script for Hexo source folder backup]]></title>
    <link href="http://hugofeng.info/2014/04/17/shell-script-for-hexo-source-folder-backup/"/>
    <id>http://hugofeng.info/2014/04/17/shell-script-for-hexo-source-folder-backup/</id>
    <published>2014-04-17T18:45:20.000Z</published>
    <updated>2014-04-21T21:36:04.000Z</updated>
    <content type="html"><![CDATA[<p><a href="http://hexo.io" target="_blank" rel="external">Hexo</a> is a very good tool to generate blog pages from mardown files, and it is more comvinient together with Github. But everytime I deploy a new post to Github using <code>hexo deploy</code>, it only commits generated .html files to the repo. </p>
<p>Those original <code>.md</code> articles however seems very important to me for revising contents in the future, and it would be troublesome if those <code>.md</code> files were somehow lost. </p>
<p>In order to keep them save and track my every changes, I decide to find a way to put them altogether in the repo with the deployed .html files. So I wrote a shell script the achieve this.</p>
<a id="more"></a>

<p>In blog project folder:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre>1
</pre></td><td class="code"><pre>$ vim <span class="keyword">push</span><span class="preprocessor">.sh</span>
</pre></td></tr></table></figure><br>add content:<br><br><figure class="highlight shell"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
9
</pre></td><td class="code"><pre><span class="shebang">#!/bin/bash
</span>
hexo generate
<span class="keyword">if</span> test <span class="operator">-d</span> public/<span class="built_in">source</span>_backup
<span class="keyword">then</span>
    rm -rf public/<span class="built_in">source</span>_backup
<span class="keyword">fi</span>
cp -r <span class="built_in">source</span>/ public/<span class="built_in">source</span>_backup
hexo deploy
</pre></td></tr></table></figure>

<p>add executable:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre>1
</pre></td><td class="code"><pre>$ <span class="keyword">chmod</span> +<span class="keyword">x</span> <span class="keyword">push</span>.sh
</pre></td></tr></table></figure>

<p>This script first generate &#x201C;ready to publish&#x201D; page files, and then check if previous version of source_backup directory exit. If exits, delete it and copy the latest version, then deploy it. <code>hexo deploy</code> command will automatically add everything in public/ folder and commit the changes.</p>
<p>Next time use this command to deploy:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre>1
</pre></td><td class="code"><pre>$ ./<span class="keyword">push</span><span class="preprocessor">.sh</span>
</pre></td></tr></table></figure>

<p>Therefore, we have our original <code>.md</code> files safely tracked in GitHub, we can easily make changes to the posts on whatever version in the past you want. You can have the script copied to other directory if you have other Hexo projects, or add them to your <code>.bash_profile</code> script.</p>
]]></content>
    <summary type="html"><![CDATA[<p><a href="http://hexo.io" target="_blank" rel="external">Hexo</a> is a very good tool to generate blog pages from mardown files, and it is more comvinient together with Github. But everytime I deploy a new post to Github using <code>hexo deploy</code>, it only commits generated .html files to the repo. </p>
<p>Those original <code>.md</code> articles however seems very important to me for revising contents in the future, and it would be troublesome if those <code>.md</code> files were somehow lost. </p>
<p>In order to keep them save and track my every changes, I decide to find a way to put them altogether in the repo with the deployed .html files. So I wrote a shell script the achieve this.</p>
]]></summary>
    
      <category term="hexo" scheme="http://hugofeng.info/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Compile OpenCV 3.0 on OSX]]></title>
    <link href="http://hugofeng.info/2014/04/17/Compile%20OpenCV3.0%20on%20OSX/"/>
    <id>http://hugofeng.info/2014/04/17/Compile OpenCV3.0 on OSX/</id>
    <published>2014-04-17T15:37:43.000Z</published>
    <updated>2014-05-06T23:16:06.000Z</updated>
    <content type="html"><![CDATA[<h3 id="1-_Install">1. Install</h3>
<p>Installing OpenCV on Mac can be fairly simple&#x2014;using <code>homebrew</code>, however, <code>homebrew</code> can only install a stable version of it(currently 2.4.8.2). If you want to have a taste of OpenCV3.0, you&#x2019;ll have to build from the source. This sometimes can be very annoying[1][2]. After more than a day&#x2019;s struggle, I find a solution for my machine.</p>
<p>If you have anaconda installed, probably the vtk module has a version of 5.x, but OpenCV3.0 need vtk at least 6.1.[1]</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre>1
</pre></td><td class="code"><pre><span class="variable">$ </span>brew install vtk   <span class="comment"># This gonna take some time</span>
</pre></td></tr></table></figure>

<p>Now download the latest OpenCV source:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre>1
</pre></td><td class="code"><pre><span class="variable">$ </span>git clone git<span class="variable">@github</span>.<span class="symbol">com:</span><span class="constant">Itseez</span>/opencv.git
</pre></td></tr></table></figure>

<p>Go to OpenCV source folder, </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre>1
2
3
</pre></td><td class="code"><pre><span class="variable">$ </span>mkdir build
<span class="variable">$ </span>cd build
<span class="variable">$ </span>cmake <span class="string">&quot;Unix Makefile&quot;</span> -<span class="constant">D</span> <span class="constant">CMAKE_OSX_ARCHITECTURES</span>=x86_64 -<span class="constant">D</span> <span class="constant">BUILD_PERF_TESTS</span>=<span class="constant">OFF</span> ..
</pre></td></tr></table></figure>

<p>Since OpenCV module &#x2018;viz&#x2019; has to be compiled with libc++ instead of libstdc++, we need to make some changes to the makefiles to ensure it is compiled that way.</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre>1
2
</pre></td><td class="code"><pre><span class="variable">$ </span>cd modules/viz/<span class="constant">CMakeFiles</span>/opencv_viz.dir/
<span class="variable">$ </span>vim flags.make
</pre></td></tr></table></figure>

<p>Add flag <code>-std=c++11 -stdlib=libc++</code><br>Do the same to <code>build/modules/viz/CMakeFiles/opencv_test_viz.dir/flags.make</code><br>Now back to the build folder. </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre>1
</pre></td><td class="code"><pre><span class="variable">$ </span>make -j8  <span class="comment"># Using 8 threads to build</span>
</pre></td></tr></table></figure>

<p>And it should work fine.<br>Now finish up:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre>1
</pre></td><td class="code"><pre><span class="variable">$ </span>make install
</pre></td></tr></table></figure>

<h3 id="2-_Check">2. Check</h3>
<p>There&#x2019;re generally 2 ways to check OpenCV&#x2019;s version. </p>
<ol>
<li><p>In the OpenCV source folder, version number is defined in file:</p>
<pre><code> modules/core/<span class="built_in">include</span>/opencv2/core/<span class="built_in">version</span>.hpp 
</code></pre></li>
<li><p>After <code>make install</code>, you can see the copied libraries, they are usually named with version numbers. Like <code>libopencv_core.3.0.0.dylib</code>.</p>
</li>
</ol>
<p>Also, you can use <code>pkg-config</code> to check if the libs are correctly located.</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre>1
</pre></td><td class="code"><pre><span class="variable">$pkg</span><span class="attribute">-config</span> <span class="subst">--</span>libs opencv
</pre></td></tr></table></figure>

<p>The output will look like this:</p>
<pre><code>/usr/<span class="built_in">local</span>/lib/libopencv_calib3d.dylib /usr/<span class="built_in">local</span>/lib/libopencv_contrib.dylib /usr/<span class="built_in">local</span>/lib/libopencv_core.dylib /usr/<span class="built_in">local</span>/lib/libopencv_cuda.dylib /usr/<span class="built_in">local</span>/lib/libopencv_cudaarithm.dylib /usr/<span class="built_in">local</span>/lib/libopencv_cudabgsegm.dylib /usr/<span class="built_in">local</span>/lib/libopencv_cudafeatures2d.dylib /usr/<span class="built_in">local</span>/lib/libopencv_cudafilters.dylib /usr/<span class="built_in">local</span>/lib/libopencv_cudaimgproc.dylib /usr/<span class="built_in">local</span>/lib/libopencv_cudaoptflow.dylib /usr/<span class="built_in">local</span>/lib/libopencv_cudastereo.dylib /usr/<span class="built_in">local</span>/lib/libopencv_cudawarping.dylib /usr/<span class="built_in">local</span>/lib/libopencv_features2d.dylib /usr/<span class="built_in">local</span>/lib/libopencv_flann.dylib /usr/<span class="built_in">local</span>/lib/libopencv_highgui.dylib /usr/<span class="built_in">local</span>/lib/libopencv_imgproc.dylib /usr/<span class="built_in">local</span>/lib/libopencv_legacy.dylib /usr/<span class="built_in">local</span>/lib/libopencv_ml.dylib /usr/<span class="built_in">local</span>/lib/libopencv_nonfree.dylib /usr/<span class="built_in">local</span>/lib/libopencv_objdetect.dylib /usr/<span class="built_in">local</span>/lib/libopencv_optim.dylib /usr/<span class="built_in">local</span>/lib/libopencv_photo.dylib /usr/<span class="built_in">local</span>/lib/libopencv_shape.dylib /usr/<span class="built_in">local</span>/lib/libopencv_softcascade.dylib /usr/<span class="built_in">local</span>/lib/libopencv_stitching.dylib /usr/<span class="built_in">local</span>/lib/libopencv_superres.dylib /usr/<span class="built_in">local</span>/lib/libopencv_ts.<span class="operator">a</span> /usr/<span class="built_in">local</span>/lib/libopencv_video.dylib /usr/<span class="built_in">local</span>/lib/libopencv_videostab.dylib /usr/<span class="built_in">local</span>/lib/libopencv_viz.dylib
</code></pre><h3 id="3-_Reference:">3. Reference:</h3>
<p>[1] <a href="http://code.opencv.org/issues/3582" target="_blank" rel="external">http://code.opencv.org/issues/3582</a><br>[2] <a href="http://stackoverflow.com/questions/19671827/opencv-installation-on-mac-os-x" target="_blank" rel="external">http://stackoverflow.com/questions/19671827/opencv-installation-on-mac-os-x</a><br>[3] <a href="http://ibivanchev.blogspot.be/2013/10/opencv-mac-os-x-109.html" target="_blank" rel="external">http://ibivanchev.blogspot.be/2013/10/opencv-mac-os-x-109.html</a></p>
]]></content>
    
    
      <category term="opencv" scheme="http://hugofeng.info/tags/opencv/"/>
    
  </entry>
  
</feed>
