<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[Flight log]]></title>
  <subtitle><![CDATA[About code, machine learning and part of Hugo's life.]]></subtitle>
  <link href="/atom.xml" rel="self"/>
  <link href="http://hugofeng.info/"/>
  <updated>2014-08-21T10:47:30.823Z</updated>
  <id>http://hugofeng.info/</id>
  
  <author>
    <name><![CDATA[Hugo Feng]]></name>
    <email><![CDATA[hugo.fxy@gmail.com]]></email>
  </author>
  
  <generator uri="http://zespia.tw/hexo/">Hexo</generator>
  
  <entry>
    <title><![CDATA[Fast Copying Timestamp on Mac]]></title>
    <link href="http://hugofeng.info/2014/08/18/Auto_Timestamp/"/>
    <id>http://hugofeng.info/2014/08/18/Auto_Timestamp/</id>
    <published>2014-08-18T16:56:19.000Z</published>
    <updated>2014-08-21T10:36:39.000Z</updated>
    <content type="html"><![CDATA[<p>When I&#x2019;m using Googe Docs to write logs for my lab work, it&#x2019;ll be much more convenient to insert timestamp and I&#x2019;m surprised that they don&#x2019;t have this feature (Microsoft Word has it). </p>
<p>But if you are using Mac OSX, this would be very easy to achive. Since there&#x2019;s a pretty good command line tool called <code>date</code> that can print timestamp in multiple formats. The thing we need to do is to copy the standard output to the clipboard. </p>
<p>There are 2 ways to do this:</p>
<ul>
<li>Using <code>Alfred</code>. <code>Alfred</code> is a tool to quick launch applications and it has a feature of quick run customized shell scripts. However, this feature is together with its <code>Power Pack</code>, which is limited to paid users. </li>
<li>Using <code>automator</code>. <code>Automator</code> is a tool provided by Mac OSX that can pack scripts or recorded actions into an application or service. Here we choose this method.</li>
</ul>
<p>Step:</p>
<ol>
<li>launch <code>Automator</code> in <code>Launcher</code>. </li>
<li>In menu bar, choose <code>File</code> -&gt; <code>New</code>, then choose <code>Application</code>.</li>
<li>In the search bar search for <code>Run Shell Script</code>.</li>
<li><p>In the right window, put in the following script:</p>
<pre><code> <span class="built_in">date</span> <span class="subst">|</span>tr <span class="attribute">-d</span> <span class="string">&apos;\n&apos;</span><span class="subst">|</span> pbcopy
</code></pre></li>
<li><p>In menu bar, choose <code>File</code> -&gt; <code>Save</code> to save your application, better to save it to <code>/Applications</code> for <code>Spotlight</code> to index. Here we name it as <code>TimeStamp</code>.</p>
</li>
</ol>
<p>Voila! Now when you are using Google Docs or any other text editor, use <code>Spotlight</code> or <code>Alfred</code> to launch the application we just created, and press key <code>Command</code> + <code>V</code> to paste the timestamp to the document. You can also download my application here.<a href="/files/TimeStamp.app">Download my TimeStamp.app</a></p>
<p>Explaining the script:</p>
<p>First <code>date</code> command will execute the <code>date</code> program and its output will be redirected to <code>tr</code> program, which with argument <code>-d &apos;\n&apos;</code>, will delete the last &#x201C;endline&#x201D; character. At last, the result will be past to <code>pbcopy</code> program to add it to the clipboard.</p>
<p>Further more, we can also add arguments to <code>date</code> to customize the format of the timestamp. Here is a nice article about this. <a href="http://www.cyberciti.biz/faq/linux-unix-formatting-dates-for-display/" target="_blank" rel="external">Link.</a> For myself, I use this <code>date +&quot;%d/%m/%y %a %H:%M&quot;</code>, which print something like this: <code>18/08/14 Mon 20:25</code> .</p>
]]></content>
    
    
      <category term="Mac shell" scheme="http://hugofeng.info/tags/Mac-shell/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[An implementation of Neural Network with Back Propagation in MATLAB]]></title>
    <link href="http://hugofeng.info/2014/06/05/bp_ann_in_matlab/"/>
    <id>http://hugofeng.info/2014/06/05/bp_ann_in_matlab/</id>
    <published>2014-06-05T20:45:14.000Z</published>
    <updated>2014-06-07T20:13:03.000Z</updated>
    <content type="html"><![CDATA[<h3 id="Test_the_program_with_a_data_set">Test the program with a data set</h3>
<ul>
<li><strong>Network attributes</strong>: 1 hidden layer with 3 neurons, initial biases and weights are set to random from -1 to 1, with 0.5 learning rate</li>
<li><strong>Target</strong>: XOR(x1, x2)</li>
<li><strong>Training</strong> set: [x1, x2] = [0 0; 0 1; 1 0; 1 1], y = [0; 1; 1; 0]</li>
<li><strong>Test set</strong>: (Same as training set),   error = (sum(errors .^ 2))^0.5</li>
</ul>
<h4 id="Output">Output</h4>
<p><img src="/img/bp1.png" alt="Standard diviation of errors with iterations"><br><img src="/img/bp2.png" alt="Program output"><br><img src="/img/bp3.png" alt="Output of trained Neural Network"></p>
<a id="more"></a>

<h3 id="Analyze">Analyze</h3>
<p>Since neural network with activation function Sigmoid, is a highly non-linear system, the formation of the network can be depended on the initial value of weights and biases, and its non-linear feature may grow with the number of layers as well as number of neurons. So sometimes, the following network formation can also be trained from the same training set and settings.</p>
<p><img src="/img/bp4.png" alt="Another trained Network1"><br><img src="/img/bp5.png" alt="Another trained Network2"></p>
<h3 id="Source_Code_in_MATLAB">Source Code in MATLAB</h3>
<figure class="highlight MATLAB"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
</pre></td><td class="code"><pre><span class="comment">%% BPANN: Artificial Neural Network with Back Propagation</span>
<span class="comment">%% Author: Xuyang Feng</span>
<span class="function"><span class="keyword">function</span>  <span class="title">BPANN</span><span class="params">()</span></span>

    <span class="comment">%---Set training parameters</span>
    iterations = <span class="number">5000</span>;
    errorThreshhold = <span class="number">0.1</span>;
    learningRate = <span class="number">0.5</span>;
    <span class="comment">%---Set hidden layer type, for example: [4, 3, 2]</span>
    hiddenNeurons = <span class="matrix">[<span class="number">3</span> <span class="number">2</span>]</span>;

    
    <span class="comment">%---&apos;Xor&apos; training data</span>
    trainInp = <span class="matrix">[<span class="number">0</span> <span class="number">0</span>; <span class="number">0</span> <span class="number">1</span>; <span class="number">1</span> <span class="number">0</span>; <span class="number">1</span> <span class="number">1</span>]</span>;
    trainOut = <span class="matrix">[<span class="number">0</span>; <span class="number">1</span>; <span class="number">1</span>; <span class="number">0</span>]</span>;
    testInp = trainInp;
    testRealOut = trainOut;
    

    <span class="comment">% %---&apos;And&apos; training data</span>
    <span class="comment">% trainInp = [1 1; 1 0; 0 1; 0 0];</span>
    <span class="comment">% trainOut = [1; 0; 0; 0];</span>
    <span class="comment">% testInp = trainInp;</span>
    <span class="comment">% testRealOut = trainOut;</span>

    assert(<span class="built_in">size</span>(trainInp,<span class="number">1</span>)==<span class="built_in">size</span>(trainOut, <span class="number">1</span>),...
        <span class="string">&apos;Counted different sets of input and output.&apos;</span>);

    <span class="comment">%---Initialize Network attributes</span>
    inArgc = <span class="built_in">size</span>(trainInp, <span class="number">2</span>);
    outArgc = <span class="built_in">size</span>(trainOut, <span class="number">2</span>);
    trainsetCount = <span class="built_in">size</span>(trainInp, <span class="number">1</span>);
    
    <span class="comment">%---Add output layer</span>
    layerOfNeurons = <span class="matrix">[hiddenNeurons, outArgc]</span>;
    layerCount = <span class="built_in">size</span>(layerOfNeurons, <span class="number">2</span>);
    
    <span class="comment">%---Weight and bias random range</span>
    e = <span class="number">1</span>;
    b = -e;

    <span class="comment">%---Set initial random weights</span>
    weightCell = cell(<span class="number">1</span>, layerCount);
    <span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:layerCount
        <span class="keyword">if</span> <span class="built_in">i</span> == <span class="number">1</span>
            weightCell<span class="cell">{<span class="number">1</span>}</span> = unifrnd(b, e, inArgc,layerOfNeurons(<span class="number">1</span>));
        <span class="keyword">else</span>
            weightCell<span class="cell">{i}</span> = unifrnd(b, e, layerOfNeurons(<span class="built_in">i</span>-<span class="number">1</span>),layerOfNeurons(<span class="built_in">i</span>));
        <span class="keyword">end</span>
    <span class="keyword">end</span>

    <span class="comment">%---Set initial biases</span>
    biasCell = cell(<span class="number">1</span>, layerCount);
    <span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:layerCount
        biasCell<span class="cell">{i}</span> = unifrnd(b, e, <span class="number">1</span>, layerOfNeurons(<span class="built_in">i</span>));
    <span class="keyword">end</span>


    <span class="comment">%----------------------</span>
    <span class="comment">%---Begin training</span>
    <span class="comment">%----------------------</span>
    <span class="keyword">for</span> iter = <span class="number">1</span>:iterations
        <span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:trainsetCount
            <span class="comment">% choice = randi([1 trainsetCount]);</span>
            choice = <span class="built_in">i</span>;
            sampleIn = trainInp(choice, :);
            sampleTarget = trainOut(choice, :);

            <span class="matrix">[realOutput, layerOutputCells]</span> = ForwardNetwork(sampleIn, layerOfNeurons, weightCell, biasCell);
            <span class="matrix">[weightCell, biasCell]</span> = BackPropagate(learningRate, sampleIn, realOutput, sampleTarget, layerOfNeurons, ...
                weightCell, biasCell, layerOutputCells);
        <span class="keyword">end</span>

        <span class="comment">%plot overall network error at end of each iteration</span>
        error = <span class="built_in">zeros</span>(trainsetCount, outArgc);
        <span class="keyword">for</span> t = <span class="number">1</span>:trainsetCount
            <span class="matrix">[predict, layeroutput]</span> = ForwardNetwork(trainInp(t, :), layerOfNeurons, weightCell, biasCell);
            p(t) = predict;
            error(t, : ) = predict - trainOut(t, :);
        <span class="keyword">end</span>

        err(iter) = (sum(<span class="transposed_variable">error.</span>^<span class="number">2</span>)/trainsetCount)^<span class="number">0.5</span>;
        figure(<span class="number">1</span>);
        plot(err);

        <span class="comment">%---Stop if reach error threshold</span>
        <span class="keyword">if</span> err(iter) &lt; errorThreshhold
            <span class="keyword">break</span>;
        <span class="keyword">end</span>
    <span class="keyword">end</span>
    
    <span class="comment">%--Test the trained network with a test set</span>
    testsetCount = <span class="built_in">size</span>(testInp, <span class="number">1</span>);
    error = <span class="built_in">zeros</span>(testsetCount, outArgc);
    <span class="keyword">for</span> t = <span class="number">1</span>:testsetCount
        <span class="matrix">[predict, layeroutput]</span> = ForwardNetwork(testInp(t, :), layerOfNeurons, weightCell, biasCell);
        p(t) = predict;
        error(t, : ) = predict - testRealOut(t, :);
    <span class="keyword">end</span>

    <span class="comment">%---Print predictions</span>
    fprintf(<span class="string">&apos;Ended with %d iterations.\n&apos;</span>, iter);
    a = testInp;
    b = testRealOut;
    c = <span class="transposed_variable">p&apos;</span>;
    x1_x2_act_pred_err = <span class="matrix">[a b c c-b]</span>

    <span class="comment">%---Plot Surface of network predictions</span>
    testInpx1 = <span class="matrix">[-<span class="number">1</span>:<span class="number">0.1</span>:<span class="number">1</span>]</span>;
    testInpx2 = <span class="matrix">[-<span class="number">1</span>:<span class="number">0.1</span>:<span class="number">1</span>]</span>;
    <span class="matrix">[X1, X2]</span> = <span class="built_in">meshgrid</span>(testInpx1, testInpx2);
    testOutRows = <span class="built_in">size</span>(X1, <span class="number">1</span>);
    testOutCols = <span class="built_in">size</span>(X1, <span class="number">2</span>);
    testOut = <span class="built_in">zeros</span>(testOutRows, testOutCols);
    <span class="keyword">for</span> row = <span class="matrix">[<span class="number">1</span>:testOutRows]</span>
        <span class="keyword">for</span> col = <span class="matrix">[<span class="number">1</span>:testOutCols]</span>
            test = <span class="matrix">[X1(row, col), X2(row, col)]</span>;
            <span class="matrix">[out, l]</span> = ForwardNetwork(test, layerOfNeurons, weightCell, biasCell);
            testOut(row, col) = out;
        <span class="keyword">end</span>
    <span class="keyword">end</span>
    figure(<span class="number">2</span>);
    surf(X1, X2, testOut);

<span class="keyword">end</span>

<span class="comment">%% BackPropagate: Backpropagate the output through the network and adjust weights and biases</span>
<span class="function"><span class="keyword">function</span> <span class="params">[weightCell, biasCell]</span> = <span class="title">BackPropagate</span><span class="params">(rate, in, realOutput, sampleTarget, layer, weightCell, biasCell, layerOutputCells)</span></span>
    layerCount = <span class="built_in">size</span>(layer, <span class="number">2</span>);
    delta = cell(<span class="number">1</span>, layerCount);
    D_weight = cell(<span class="number">1</span>, layerCount);
    D_bias = cell(<span class="number">1</span>, layerCount);
    <span class="comment">%---From Output layer, it has different formula</span>
    output = layerOutputCells<span class="cell">{layerCount}</span>;
    delta<span class="cell">{layerCount}</span> = output .* (<span class="number">1</span>-output) .* (sampleTarget - output);
    preoutput = layerOutputCells<span class="cell">{layerCount-<span class="number">1</span>}</span>;
    D_weight<span class="cell">{layerCount}</span> = rate .* <span class="transposed_variable">preoutput&apos;</span> * delta<span class="cell">{layerCount}</span>;
    D_bias<span class="cell">{layerCount}</span> = rate .* delta<span class="cell">{layerCount}</span>;

    <span class="comment">%---Back propagate for Hidden layers</span>
    <span class="keyword">for</span> layerIndex = layerCount-<span class="number">1</span>:-<span class="number">1</span>:<span class="number">1</span>
        output = layerOutputCells<span class="cell">{layerIndex}</span>;
        <span class="keyword">if</span> layerIndex == <span class="number">1</span>
            preoutput = in;
        <span class="keyword">else</span>
            preoutput = layerOutputCells<span class="cell">{layerIndex-<span class="number">1</span>}</span>;
        <span class="keyword">end</span>

        weight = weightCell<span class="cell">{layerIndex+<span class="number">1</span>}</span>;
        sumup = (weight * delta<span class="cell">{layerIndex+<span class="number">1</span>}&apos;</span>)<span class="string">&apos;;
        delta{layerIndex} = output .* (1 - output) .* sumup;

        D_weight{layerIndex} = rate .* preoutput&apos;</span> * delta<span class="cell">{layerIndex}</span>;
        D_bias<span class="cell">{layerIndex}</span> = rate .* delta<span class="cell">{layerIndex}</span>;
    <span class="keyword">end</span>
    <span class="comment">%---Update weightCell and biasCell</span>
    <span class="keyword">for</span> layerIndex = <span class="number">1</span>:layerCount
        weightCell<span class="cell">{layerIndex}</span> = weightCell<span class="cell">{layerIndex}</span> + D_weight<span class="cell">{layerIndex}</span>;
        biasCell<span class="cell">{layerIndex}</span> = biasCell<span class="cell">{layerIndex}</span> + D_bias<span class="cell">{layerIndex}</span>;
    <span class="keyword">end</span>


<span class="keyword">end</span>


<span class="comment">%% ForwardNetwork: Compute feed forward neural network, Return the output and output of each neuron in each layer</span>
<span class="function"><span class="keyword">function</span> <span class="params">[realOutput, layerOutputCells]</span> = <span class="title">ForwardNetwork</span><span class="params">(in, layer, weightCell, biasCell)</span></span>
    layerCount = <span class="built_in">size</span>(layer, <span class="number">2</span>);
    layerOutputCells = cell(<span class="number">1</span>, layerCount);

    out = in;
    <span class="keyword">for</span> layerIndex = <span class="number">1</span>:layerCount
        X = out;
        bias = biasCell<span class="cell">{layerIndex}</span>;
        out = Sigmoid(X * weightCell<span class="cell">{layerIndex}</span> + bias);
        layerOutputCells<span class="cell">{layerIndex}</span> = out;
    <span class="keyword">end</span>
    realOutput = out;    
<span class="keyword">end</span>
</pre></td></tr></table></figure>
]]></content>
    <summary type="html"><![CDATA[<h3 id="Test_the_program_with_a_data_set">Test the program with a data set</h3>
<ul>
<li><strong>Network attributes</strong>: 1 hidden layer with 3 neurons, initial biases and weights are set to random from -1 to 1, with 0.5 learning rate</li>
<li><strong>Target</strong>: XOR(x1, x2)</li>
<li><strong>Training</strong> set: [x1, x2] = [0 0; 0 1; 1 0; 1 1], y = [0; 1; 1; 0]</li>
<li><strong>Test set</strong>: (Same as training set),   error = (sum(errors .^ 2))^0.5</li>
</ul>
<h4 id="Output">Output</h4>
<p><img src="/img/bp1.png" alt="Standard diviation of errors with iterations"><br><img src="/img/bp2.png" alt="Program output"><br><img src="/img/bp3.png" alt="Output of trained Neural Network"></p>
]]></summary>
    
      <category term="ML" scheme="http://hugofeng.info/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Floating point operations on ARM processor]]></title>
    <link href="http://hugofeng.info/2014/04/25/float-operations-on-arm/"/>
    <id>http://hugofeng.info/2014/04/25/float-operations-on-arm/</id>
    <published>2014-04-25T19:32:34.000Z</published>
    <updated>2014-05-06T23:15:42.000Z</updated>
    <content type="html"><![CDATA[<p>This article is a brief introduction of different implementations of floating point operation on an ARM processor.</p>
<h3 id="1-_Early_ages">1. Early ages</h3>
<p>There&#x2019;s no coprocessor for floating point operations on ARM, those operations are done by CPU, means of which is called <code>Float Math Emulation</code>. Normally each float point operation costs thousands of cycles, which is very inefficient. </p>
<h3 id="2-_Soft-float">2. Soft-float</h3>
<p>This is for those situations where the CPU does not contain a <code>floating point unit(FPU)</code>, which is a built-in coprocessor on CPU to deal with floating point operations. </p>
<p>There are 2 different ways to do floating point operations without FPU: </p>
<ol>
<li><p>The kernel will trap all the opcodes related to the FPU and then perform the calculations itself [1]. This is called <code>NWFPE</code> (NetWinder Floating Point Emulator), however, this has now been removed from ARM kernel [2]. As I mentioned before, this is the old fashioned way of implementation, which involves raising of invalid instruction exceptions each time the situation occurs [3]. And this can be very expensive for computation. </p>
</li>
<li><p>All floating point operations are translated to specific inline function calls by compiler. This is done by passing flag <code>-mfloat-abi=soft</code> when using <code>gcc</code>, and gcc will use built in software(library) to emulate.</p>
</li>
</ol>
<h3 id="3-_Hard-float">3. Hard-float</h3>
<p>Nowadays, many chips has hardware <code>FPU</code> support to accelerate fp operations, for ARM familis, this unit is often called <code>VFP</code> (Vector Floating-Point coprocessor). VFP is a fully IEEE-754 compatible floating point unit. But later a much more powerful NEON Advanced SIMD unit was introduced and is suggested instead of VFP by Architecture Reference Manual.</p>
<a id="more"></a>

<h4 id="3-1_VFP">3.1 VFP</h4>
<p>One of the nice feature VFP provides is that it supports single and double-precision arithmetic on vector-vector, vector-scalar, and scalar-scalar data sets where vectors can consist of up to 8 single-precision, or 4 double-precision elements [4]. The &#x201C;vector mode&#x201D; instructions of VFP is actually sequential, so the speed up is very limited [8]. There&#x2019;s one thing worth mentioning, that the &#x201C;vector mode&#x201D; of VFP is actually deprecated, replaced shortly after its introduce, with the much more powerful NEON Advanced SIMD unit [9]. </p>
<p>To use VFP, one need to pass flag <code>-mfloat-abi=softfp</code> or <code>-mfloat-abi=hard</code> to gcc when compiling.</p>
<h5 id="3-1-1_Difference_between_softfp_and_hard_mode">3.1.1 Difference between <code>softfp</code> and <code>hard</code> mode</h5>
<ul>
<li><p>For <code>softfp</code> mode:<br>Using general integer registers to pass values (like the <code>soft</code> mode). It also support linking to <code>soft</code> mode compiled binaris. </p>
</li>
<li><p>For <code>hard</code> mode:<br>Using floating point registers on FPU to pass values. Doesn&#x2019;t support linking to <code>soft</code> mode binaris. All codes must be compiled in <code>hard</code> mode. It saves up to 20 cycles when calling a function with fp arguments, therefore faster than <code>softfp</code> mode [6].</p>
</li>
</ul>
<p>Nowadays, ARM Linux is set to <code>hard</code> mode by default [3] while Debian Armel set <code>softfp</code> as default [7]. And these two modes are not compatible in one application due to different value passing conventions. </p>
<p>Besides, <code>-msoft-float</code> equals to <code>-mfloat-abi=soft</code>, so as <code>-mhard-float</code> to <code>-mfloat-abi=hard</code> [8].</p>
<h4 id="3-2_New_era,_with_Neon">3.2 New era, with Neon</h4>
<p>The Advanced SIMD extension (aka NEON or &#x201C;MPE&#x201D; Media Processing Engine) is a combined 64- and 128-bit SIMD instruction set that provides standardized acceleration for media and signal processing applications. NEON is included in all Cortex-A8 devices but is optional in Cortex-A9 devices [9]. </p>
<p>Neon shares the same floating point registers with VFP, and thanks to floating point pipeline technology that Neon supports, it is a lot faster than VFP, but the draw back is that the NEON floating point pipeline is not entirely IEEE-754 compliant[10]. </p>
<p>Following is quoted from Peter on <a href="http://stackoverflow.com/questions/4097034/arm-cortex-a8-whats-the-difference-between-vfp-and-neon" target="_blank" rel="external">StackOverflow</a>. He&#x2019;s got a good introduction on this.</p>
<blockquote>
<p>Because it is not IEEE-754 compliant, a compiler cannot generate these instructions unless you tell the compiler that you are not interested in full compliance. This can be done in several ways.</p>
<ol>
<li><p>Using an intrinsic function to force NEON usage, for example see the <a href="http://gcc.gnu.org/onlinedocs/gcc/ARM-NEON-Intrinsics.html" target="_blank" rel="external">GCC Neon Intrinsic Function List</a>.</p>
</li>
<li><p>Ask the compiler, very nicely. Even newer GCC versions with <code>-mfpu=neon</code> will not generate floating point NEON instructions unless you also specify <code>-funsafe-math-optimizations</code>.</p>
</li>
</ol>
</blockquote>
<h3 id="4-_Conclusion">4. Conclusion</h3>
<p>ARM chips is not specifically designed for floating point operations, but with the help of coprocessors, the speed of these expensive operations can be greatly improved.</p>
<h3 id="5-_Reference">5. Reference</h3>
<ul>
<li>[1] <a href="http://www.nslu2-linux.org/wiki/FAQ/SoftHardFloatCompiler" target="_blank" rel="external">http://www.nslu2-linux.org/wiki/FAQ/SoftHardFloatCompiler</a></li>
<li>[2] <a href="http://lwn.net/Articles/546840/" target="_blank" rel="external">http://lwn.net/Articles/546840/</a></li>
<li>[3] <a href="http://linux-7110.sourceforge.net/howtos/netbook_new/x1114.htm" target="_blank" rel="external">http://linux-7110.sourceforge.net/howtos/netbook_new/x1114.htm</a></li>
<li>[4] <a href="http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.ddi0301h/Cegdejjh.html" target="_blank" rel="external">http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.ddi0301h/Cegdejjh.html</a></li>
<li>[5] <a href="http://www.gurucoding.com/en/rpi_cross_compiler/diff_hardfp_softfp.php" target="_blank" rel="external">http://www.gurucoding.com/en/rpi_cross_compiler/diff_hardfp_softfp.php</a></li>
<li>[6] <a href="https://community.freescale.com/thread/219966" target="_blank" rel="external">https://community.freescale.com/thread/219966</a></li>
<li>[7] <a href="https://wiki.debian.org/ArmEabiPort" target="_blank" rel="external">https://wiki.debian.org/ArmEabiPort</a></li>
<li>[8] <a href="http://gcc.gnu.org/onlinedocs/gcc-4.1.1/gcc/ARM-Options.html#ARM-Options" target="_blank" rel="external">http://gcc.gnu.org/onlinedocs/gcc-4.1.1/gcc/ARM-Options.html#ARM-Options</a></li>
<li>[9] <a href="http://en.wikipedia.org/wiki/ARM_architecture" target="_blank" rel="external">http://en.wikipedia.org/wiki/ARM_architecture</a></li>
<li>[10] <a href="http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.dui0204h/Bcfhfbga.html" target="_blank" rel="external">http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.dui0204h/Bcfhfbga.html</a></li>
</ul>
]]></content>
    <summary type="html"><![CDATA[<p>This article is a brief introduction of different implementations of floating point operation on an ARM processor.</p>
<h3 id="1-_Early_ages">1. Early ages</h3>
<p>There&#x2019;s no coprocessor for floating point operations on ARM, those operations are done by CPU, means of which is called <code>Float Math Emulation</code>. Normally each float point operation costs thousands of cycles, which is very inefficient. </p>
<h3 id="2-_Soft-float">2. Soft-float</h3>
<p>This is for those situations where the CPU does not contain a <code>floating point unit(FPU)</code>, which is a built-in coprocessor on CPU to deal with floating point operations. </p>
<p>There are 2 different ways to do floating point operations without FPU: </p>
<ol>
<li><p>The kernel will trap all the opcodes related to the FPU and then perform the calculations itself [1]. This is called <code>NWFPE</code> (NetWinder Floating Point Emulator), however, this has now been removed from ARM kernel [2]. As I mentioned before, this is the old fashioned way of implementation, which involves raising of invalid instruction exceptions each time the situation occurs [3]. And this can be very expensive for computation. </p>
</li>
<li><p>All floating point operations are translated to specific inline function calls by compiler. This is done by passing flag <code>-mfloat-abi=soft</code> when using <code>gcc</code>, and gcc will use built in software(library) to emulate.</p>
</li>
</ol>
<h3 id="3-_Hard-float">3. Hard-float</h3>
<p>Nowadays, many chips has hardware <code>FPU</code> support to accelerate fp operations, for ARM familis, this unit is often called <code>VFP</code> (Vector Floating-Point coprocessor). VFP is a fully IEEE-754 compatible floating point unit. But later a much more powerful NEON Advanced SIMD unit was introduced and is suggested instead of VFP by Architecture Reference Manual.</p>
]]></summary>
    
      <category term="embedded" scheme="http://hugofeng.info/tags/embedded/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Shell Script for Hexo source folder backup]]></title>
    <link href="http://hugofeng.info/2014/04/17/shell-script-for-hexo-source-folder-backup/"/>
    <id>http://hugofeng.info/2014/04/17/shell-script-for-hexo-source-folder-backup/</id>
    <published>2014-04-17T18:45:20.000Z</published>
    <updated>2014-04-21T21:36:04.000Z</updated>
    <content type="html"><![CDATA[<p><a href="http://hexo.io" target="_blank" rel="external">Hexo</a> is a very good tool to generate blog pages from mardown files, and it is more comvinient together with Github. But everytime I deploy a new post to Github using <code>hexo deploy</code>, it only commits generated .html files to the repo. </p>
<p>Those original <code>.md</code> articles however seems very important to me for revising contents in the future, and it would be troublesome if those <code>.md</code> files were somehow lost. </p>
<p>In order to keep them save and track my every changes, I decide to find a way to put them altogether in the repo with the deployed .html files. So I wrote a shell script the achieve this.</p>
<a id="more"></a>

<p>In blog project folder:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre>1
</pre></td><td class="code"><pre>$ vim <span class="keyword">push</span><span class="preprocessor">.sh</span>
</pre></td></tr></table></figure><br>add content:<br><br><figure class="highlight shell"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
9
</pre></td><td class="code"><pre><span class="shebang">#!/bin/bash
</span>
hexo generate
<span class="keyword">if</span> test <span class="operator">-d</span> public/<span class="built_in">source</span>_backup
<span class="keyword">then</span>
    rm -rf public/<span class="built_in">source</span>_backup
<span class="keyword">fi</span>
cp -r <span class="built_in">source</span>/ public/<span class="built_in">source</span>_backup
hexo deploy
</pre></td></tr></table></figure>

<p>add executable:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre>1
</pre></td><td class="code"><pre>$ <span class="keyword">chmod</span> +<span class="keyword">x</span> <span class="keyword">push</span>.sh
</pre></td></tr></table></figure>

<p>This script first generate &#x201C;ready to publish&#x201D; page files, and then check if previous version of source_backup directory exit. If exits, delete it and copy the latest version, then deploy it. <code>hexo deploy</code> command will automatically add everything in public/ folder and commit the changes.</p>
<p>Next time use this command to deploy:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre>1
</pre></td><td class="code"><pre>$ ./<span class="keyword">push</span><span class="preprocessor">.sh</span>
</pre></td></tr></table></figure>

<p>Therefore, we have our original <code>.md</code> files safely tracked in GitHub, we can easily make changes to the posts on whatever version in the past you want. You can have the script copied to other directory if you have other Hexo projects, or add them to your <code>.bash_profile</code> script.</p>
]]></content>
    <summary type="html"><![CDATA[<p><a href="http://hexo.io" target="_blank" rel="external">Hexo</a> is a very good tool to generate blog pages from mardown files, and it is more comvinient together with Github. But everytime I deploy a new post to Github using <code>hexo deploy</code>, it only commits generated .html files to the repo. </p>
<p>Those original <code>.md</code> articles however seems very important to me for revising contents in the future, and it would be troublesome if those <code>.md</code> files were somehow lost. </p>
<p>In order to keep them save and track my every changes, I decide to find a way to put them altogether in the repo with the deployed .html files. So I wrote a shell script the achieve this.</p>
]]></summary>
    
      <category term="hexo" scheme="http://hugofeng.info/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Compile OpenCV 3.0 on OSX]]></title>
    <link href="http://hugofeng.info/2014/04/17/Compile%20OpenCV3.0%20on%20OSX/"/>
    <id>http://hugofeng.info/2014/04/17/Compile OpenCV3.0 on OSX/</id>
    <published>2014-04-17T15:37:43.000Z</published>
    <updated>2014-05-06T23:16:06.000Z</updated>
    <content type="html"><![CDATA[<h3 id="1-_Install">1. Install</h3>
<p>Installing OpenCV on Mac can be fairly simple&#x2014;using <code>homebrew</code>, however, <code>homebrew</code> can only install a stable version of it(currently 2.4.8.2). If you want to have a taste of OpenCV3.0, you&#x2019;ll have to build from the source. This sometimes can be very annoying[1][2]. After more than a day&#x2019;s struggle, I find a solution for my machine.</p>
<p>If you have anaconda installed, probably the vtk module has a version of 5.x, but OpenCV3.0 need vtk at least 6.1.[1]</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre>1
</pre></td><td class="code"><pre><span class="variable">$ </span>brew install vtk   <span class="comment"># This gonna take some time</span>
</pre></td></tr></table></figure>

<p>Now download the latest OpenCV source:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre>1
</pre></td><td class="code"><pre><span class="variable">$ </span>git clone git<span class="variable">@github</span>.<span class="symbol">com:</span><span class="constant">Itseez</span>/opencv.git
</pre></td></tr></table></figure>

<p>Go to OpenCV source folder, </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre>1
2
3
</pre></td><td class="code"><pre><span class="variable">$ </span>mkdir build
<span class="variable">$ </span>cd build
<span class="variable">$ </span>cmake <span class="string">&quot;Unix Makefile&quot;</span> -<span class="constant">D</span> <span class="constant">CMAKE_OSX_ARCHITECTURES</span>=x86_64 -<span class="constant">D</span> <span class="constant">BUILD_PERF_TESTS</span>=<span class="constant">OFF</span> ..
</pre></td></tr></table></figure>

<p>Since OpenCV module &#x2018;viz&#x2019; has to be compiled with libc++ instead of libstdc++, we need to make some changes to the makefiles to ensure it is compiled that way.</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre>1
2
</pre></td><td class="code"><pre><span class="variable">$ </span>cd modules/viz/<span class="constant">CMakeFiles</span>/opencv_viz.dir/
<span class="variable">$ </span>vim flags.make
</pre></td></tr></table></figure>

<p>Add flag <code>-std=c++11 -stdlib=libc++</code><br>Do the same to <code>build/modules/viz/CMakeFiles/opencv_test_viz.dir/flags.make</code><br>Now back to the build folder. </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre>1
</pre></td><td class="code"><pre><span class="variable">$ </span>make -j8  <span class="comment"># Using 8 threads to build</span>
</pre></td></tr></table></figure>

<p>And it should work fine.<br>Now finish up:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre>1
</pre></td><td class="code"><pre><span class="variable">$ </span>make install
</pre></td></tr></table></figure>

<h3 id="2-_Check">2. Check</h3>
<p>There&#x2019;re generally 2 ways to check OpenCV&#x2019;s version. </p>
<ol>
<li><p>In the OpenCV source folder, version number is defined in file:</p>
<pre><code> modules/core/<span class="built_in">include</span>/opencv2/core/<span class="built_in">version</span>.hpp 
</code></pre></li>
<li><p>After <code>make install</code>, you can see the copied libraries, they are usually named with version numbers. Like <code>libopencv_core.3.0.0.dylib</code>.</p>
</li>
</ol>
<p>Also, you can use <code>pkg-config</code> to check if the libs are correctly located.</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre>1
</pre></td><td class="code"><pre><span class="variable">$pkg</span><span class="attribute">-config</span> <span class="subst">--</span>libs opencv
</pre></td></tr></table></figure>

<p>The output will look like this:</p>
<pre><code>/usr/<span class="built_in">local</span>/lib/libopencv_calib3d.dylib /usr/<span class="built_in">local</span>/lib/libopencv_contrib.dylib /usr/<span class="built_in">local</span>/lib/libopencv_core.dylib /usr/<span class="built_in">local</span>/lib/libopencv_cuda.dylib /usr/<span class="built_in">local</span>/lib/libopencv_cudaarithm.dylib /usr/<span class="built_in">local</span>/lib/libopencv_cudabgsegm.dylib /usr/<span class="built_in">local</span>/lib/libopencv_cudafeatures2d.dylib /usr/<span class="built_in">local</span>/lib/libopencv_cudafilters.dylib /usr/<span class="built_in">local</span>/lib/libopencv_cudaimgproc.dylib /usr/<span class="built_in">local</span>/lib/libopencv_cudaoptflow.dylib /usr/<span class="built_in">local</span>/lib/libopencv_cudastereo.dylib /usr/<span class="built_in">local</span>/lib/libopencv_cudawarping.dylib /usr/<span class="built_in">local</span>/lib/libopencv_features2d.dylib /usr/<span class="built_in">local</span>/lib/libopencv_flann.dylib /usr/<span class="built_in">local</span>/lib/libopencv_highgui.dylib /usr/<span class="built_in">local</span>/lib/libopencv_imgproc.dylib /usr/<span class="built_in">local</span>/lib/libopencv_legacy.dylib /usr/<span class="built_in">local</span>/lib/libopencv_ml.dylib /usr/<span class="built_in">local</span>/lib/libopencv_nonfree.dylib /usr/<span class="built_in">local</span>/lib/libopencv_objdetect.dylib /usr/<span class="built_in">local</span>/lib/libopencv_optim.dylib /usr/<span class="built_in">local</span>/lib/libopencv_photo.dylib /usr/<span class="built_in">local</span>/lib/libopencv_shape.dylib /usr/<span class="built_in">local</span>/lib/libopencv_softcascade.dylib /usr/<span class="built_in">local</span>/lib/libopencv_stitching.dylib /usr/<span class="built_in">local</span>/lib/libopencv_superres.dylib /usr/<span class="built_in">local</span>/lib/libopencv_ts.<span class="operator">a</span> /usr/<span class="built_in">local</span>/lib/libopencv_video.dylib /usr/<span class="built_in">local</span>/lib/libopencv_videostab.dylib /usr/<span class="built_in">local</span>/lib/libopencv_viz.dylib
</code></pre><h3 id="3-_Reference:">3. Reference:</h3>
<p>[1] <a href="http://code.opencv.org/issues/3582" target="_blank" rel="external">http://code.opencv.org/issues/3582</a><br>[2] <a href="http://stackoverflow.com/questions/19671827/opencv-installation-on-mac-os-x" target="_blank" rel="external">http://stackoverflow.com/questions/19671827/opencv-installation-on-mac-os-x</a><br>[3] <a href="http://ibivanchev.blogspot.be/2013/10/opencv-mac-os-x-109.html" target="_blank" rel="external">http://ibivanchev.blogspot.be/2013/10/opencv-mac-os-x-109.html</a></p>
]]></content>
    
    
      <category term="opencv" scheme="http://hugofeng.info/tags/opencv/"/>
    
  </entry>
  
</feed>
